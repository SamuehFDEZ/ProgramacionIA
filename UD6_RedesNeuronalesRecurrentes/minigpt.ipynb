{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Layer, Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout \nfrom tensorflow.keras.models import Model \nfrom tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:28.556319Z","iopub.execute_input":"2025-03-29T15:07:28.556602Z","iopub.status.idle":"2025-03-29T15:07:40.598672Z","shell.execute_reply.started":"2025-03-29T15:07:28.556570Z","shell.execute_reply":"2025-03-29T15:07:40.597988Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ======= 1. BLOQUE TRANSFORMER ======= \nclass TransformerBlock(Layer): \n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n        super(TransformerBlock, self).__init__() \n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) \n        self.ffn = tf.keras.Sequential([ \n        Dense(ff_dim, activation=\"relu\"), \n        Dense(embed_dim) \n        ]) \n        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n        self.dropout1 = Dropout(rate) \n        self.dropout2 = Dropout(rate) \n        \n    def call(self, inputs, training): \n        attn_output = self.att(inputs, inputs)  # Autoatención \n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output) \n        ffn_output = self.ffn(out1) \n        ffn_output = self.dropout2(ffn_output, training=training) \n        return self.layernorm2(out1 + ffn_output) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:40.599431Z","iopub.execute_input":"2025-03-29T15:07:40.599922Z","iopub.status.idle":"2025-03-29T15:07:40.605401Z","shell.execute_reply.started":"2025-03-29T15:07:40.599898Z","shell.execute_reply":"2025-03-29T15:07:40.604582Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ======= 2. MODELO TRANSFORMER ======= \nclass MiniGPT(Model): \n    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, max_len): \n        super(MiniGPT, self).__init__() \n        self.embed_dim = embed_dim \n        self.embedding = Embedding(vocab_size, embed_dim) \n        self.pos_embedding = Embedding(max_len, embed_dim) \n        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, \n        ff_dim) for _ in range(num_blocks)] \n        self.norm = LayerNormalization(epsilon=1e-6) \n        self.out_layer = Dense(vocab_size, activation=\"softmax\") \n    def call(self, inputs, training): \n        positions = tf.range(start=0, limit=tf.shape(inputs)[-1], delta=1) \n        embedded_inputs = self.embedding(inputs) + self.pos_embedding(positions) \n        x = embedded_inputs \n        for transformer_block in self.transformer_blocks: \n            x = transformer_block(x, training=training) \n        x = self.norm(x) \n        return self.out_layer(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:45.671114Z","iopub.execute_input":"2025-03-29T15:08:45.671445Z","iopub.status.idle":"2025-03-29T15:08:45.677261Z","shell.execute_reply.started":"2025-03-29T15:08:45.671419Z","shell.execute_reply":"2025-03-29T15:08:45.676496Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ======= 3. PREPARAR EL MODELO ======= \nvocab_size = 10000  # Número de palabras en el vocabulario \nembed_dim = 128  # Dimensión de los embeddings \nnum_heads = 8  # Número de cabezas de atención \nff_dim = 512  # Dimensión de la red feedforward \nnum_blocks = 4  # Número de bloques Transformer \nmax_len = 50  # Longitud máxima de la secuencia \n# Crear el modelo \ntransformer = MiniGPT(vocab_size, embed_dim, num_heads, ff_dim, num_blocks, max_len) \ntransformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\") \ntransformer.summary() \n# Simulación de datos aleatorios (¡Usar datos reales en la práctica!) \nX_train = np.random.randint(0, vocab_size, (1000, max_len))  # 1000 ejemplos \ny_train = np.random.randint(0, vocab_size, (1000, max_len)) \n# Entrenar el modelo \ntransformer.fit(X_train, y_train, batch_size=32, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:51.230693Z","iopub.execute_input":"2025-03-29T15:08:51.230975Z","iopub.status.idle":"2025-03-29T15:09:31.509920Z","shell.execute_reply.started":"2025-03-29T15:08:51.230954Z","shell.execute_reply":"2025-03-29T15:09:31.509173Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"mini_gpt\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_gpt\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block_1                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block_2                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block_3                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ layer_normalization_8                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block_1                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block_2                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ transformer_block_3                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ layer_normalization_8                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 358ms/step - loss: 9.2246\nEpoch 2/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.1643\nEpoch 3/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.1494\nEpoch 4/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.1425\nEpoch 5/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.1356\nEpoch 6/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.1351\nEpoch 7/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 9.1236\nEpoch 8/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 9.1202\nEpoch 9/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 9.1182\nEpoch 10/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.1159\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e4ce5c520e0>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"## CÓDIGO DEL PROGRAMA DE GENERACIÓN DE TEXTO BASADO EN EL TRANSFORMER \n# ======= 1. TOKENIZADOR ======= \n# Simulación de un vocabulario pequeño (en práctica usar más datos) \nvocab_size = 10000 \ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\") \ntokenizer.fit_on_texts([\"Hola, ¿cómo estás?\", \"El modelo Transformer genera texto.\", \"Aprender inteligencia artificial es emocionante.\"]) \n \n# ======= 2. FUNCIÓN PARA GENERAR TEXTO ======= \ndef generar_texto(model, seed_text, max_len=50, num_words=10): \n    for _ in range(num_words): \n        # Convertir el texto en tokens \n        token_list = tokenizer.texts_to_sequences([seed_text])[0] \n        token_list = pad_sequences([token_list], maxlen=max_len, padding='pre') \n \n        # Predecir siguiente palabra \n        predicted_probs = model.predict(token_list, verbose=0)[0][-1]  # Última palabra \n        predicted_index = np.argmax(predicted_probs)  # Elegir la palabra con mayor probabilidad \n         \n        # Convertir índice en palabra \n        for word, index in tokenizer.word_index.items(): \n            if index == predicted_index: \n                seed_text += \" \" + word \n                break \n \n    return seed_text \n \n# ======= 3. PRUEBA DE GENERACIÓN ======= \ntexto_generado = generar_texto(transformer, \"Hola, ¿cómo\", num_words=10) \nprint(\"Texto generado:\", texto_generado)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:11:22.769129Z","iopub.execute_input":"2025-03-29T15:11:22.769464Z","iopub.status.idle":"2025-03-29T15:11:25.038763Z","shell.execute_reply.started":"2025-03-29T15:11:22.769441Z","shell.execute_reply":"2025-03-29T15:11:25.037859Z"}},"outputs":[{"name":"stdout","text":"Texto generado: Hola, ¿cómo\n","output_type":"stream"}],"execution_count":8}]}