{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339662,"sourceType":"datasetVersion","datasetId":7094183}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import TextVectorization, Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:45:10.464436Z","iopub.execute_input":"2025-04-09T13:45:10.464784Z","iopub.status.idle":"2025-04-09T13:45:26.439531Z","shell.execute_reply.started":"2025-04-09T13:45:10.464731Z","shell.execute_reply":"2025-04-09T13:45:26.438580Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tuits-desastres/tuits_desastres.csv\")\ndf = df[['text', 'target']].dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:59:18.185945Z","iopub.execute_input":"2025-04-09T13:59:18.186293Z","iopub.status.idle":"2025-04-09T13:59:18.264409Z","shell.execute_reply.started":"2025-04-09T13:59:18.186266Z","shell.execute_reply":"2025-04-09T13:59:18.263413Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df['text'] = df['text'].str.lower()\ndf['text'] = df['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))             # URLs\ndf['text'] = df['text'].apply(lambda x: re.sub(r\"@\\w+\", \"\", x))                # menciones\ndf['text'] = df['text'].apply(lambda x: re.sub(r\"#\", \"\", x))                   # hashtag\ndf['text'] = df['text'].apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", x))             # puntuación\ndf['text'] = df['text'].apply(lambda x: re.sub(r\"\\s+\", \" \", x))                # múltiples espacios a uno\ndf['text'] = df['text'].str.strip()                                            # quitar espacios","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:59:20.972846Z","iopub.execute_input":"2025-04-09T13:59:20.973284Z","iopub.status.idle":"2025-04-09T13:59:21.129940Z","shell.execute_reply.started":"2025-04-09T13:59:20.973248Z","shell.execute_reply":"2025-04-09T13:59:21.128995Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"for i in range(20):\n    print(df['text'][i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:21.395614Z","iopub.execute_input":"2025-04-09T14:01:21.395975Z","iopub.status.idle":"2025-04-09T14:01:21.405655Z","shell.execute_reply.started":"2025-04-09T14:01:21.395945Z","shell.execute_reply":"2025-04-09T14:01:21.403525Z"}},"outputs":[{"name":"stdout","text":"our deeds are the reason of this earthquake may allah forgive us all\nforest fire near la ronge sask canada\nall residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected\n13000 people receive wildfires evacuation orders in california\njust got sent this photo from ruby alaska as smoke from wildfires pours into a school\nrockyfire update california hwy 20 closed in both directions due to lake county fire cafire wildfires\nflood disaster heavy rain causes flash flooding of streets in manitou colorado springs areas\nim on top of the hill and i can see a fire in the woods\ntheres an emergency evacuation happening now in the building across the street\nim afraid that the tornado is coming to our area\nthree people died from the heat wave so far\nhaha south tampa is getting flooded hah wait a second i live in south tampa what am i gonna do what am i gonna do fvck flooding\nraining flooding florida tampabay tampa 18 or 19 days ive lost count\nflood in bago myanmar we arrived bago\ndamage to school bus on 80 in multi car crash breaking\nwhats up man\ni love fruits\nsummer is lovely\nmy car is so fast\nwhat a goooooooaaaaaal\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"max_tokens = 10000 \nsequence_length = 100\n\nvectorizer = TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=sequence_length)\nvectorizer.adapt(df['text'].values)\n\nX = vectorizer(df['text'].values)\ny = df['target'].values\n\nX_train, X_val, y_train, y_val = train_test_split(X.numpy(), y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:47.255346Z","iopub.execute_input":"2025-04-09T14:01:47.255763Z","iopub.status.idle":"2025-04-09T14:01:47.528531Z","shell.execute_reply.started":"2025-04-09T14:01:47.255732Z","shell.execute_reply":"2025-04-09T14:01:47.527619Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Crear el modelo con LSTM\nmodel = Sequential([\n    Embedding(input_dim=max_tokens, output_dim=64),\n    Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n    Bidirectional(LSTM(32, dropout=0.3, recurrent_dropout=0.3)),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:00:57.984193Z","iopub.execute_input":"2025-04-09T12:00:57.984514Z","iopub.status.idle":"2025-04-09T12:00:58.024293Z","shell.execute_reply.started":"2025-04-09T12:00:57.984488Z","shell.execute_reply":"2025-04-09T12:00:58.023633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenar el modelo\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val),callbacks=[early_stop])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:01:02.033155Z","iopub.execute_input":"2025-04-09T12:01:02.033481Z","iopub.status.idle":"2025-04-09T12:04:08.508596Z","shell.execute_reply.started":"2025-04-09T12:01:02.033452Z","shell.execute_reply":"2025-04-09T12:04:08.507892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluar\nloss, accuracy = model.evaluate(X_val, y_val)\nprint(f\"Precisión en validación: {accuracy:.4f}\")\n\n# Curva de precisión\nplt.plot(history.history['accuracy'], label='Entrenamiento')\nplt.plot(history.history['val_accuracy'], label='Validación')\nplt.xlabel('Época')\nplt.ylabel('Precisión')\nplt.legend()\nplt.title('Precisión durante el entrenamiento')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:04:30.339740Z","iopub.execute_input":"2025-04-09T12:04:30.340035Z","iopub.status.idle":"2025-04-09T12:04:35.329490Z","shell.execute_reply.started":"2025-04-09T12:04:30.340014Z","shell.execute_reply":"2025-04-09T12:04:35.328610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['target'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:00:49.437023Z","iopub.status.idle":"2025-04-09T12:00:49.437270Z","shell.execute_reply":"2025-04-09T12:00:49.437165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.shape, y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:00:49.438128Z","iopub.status.idle":"2025-04-09T12:00:49.438483Z","shell.execute_reply":"2025-04-09T12:00:49.438300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plotLossAccuracy(history):\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Épocas')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Evolución de la pérdida')\n    plt.ylim(bottom=0)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Épocas')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Evolución de la precisión')\n    plt.ylim(bottom=0)\n    \n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluateModel(model):\n    y_pred = (model.predict([q1_test, q2_test]) > 0.5).astype(int)\n    \n    conf_matrix = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(4, 4))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Duplicada', 'Duplicada'], yticklabels=['No Duplicada', 'Duplicada'])\n    plt.xlabel('Predicción')\n    plt.ylabel('Real')\n    plt.title('Matriz de confusión')\n    plt.show()\n    \n    print(\"Informe de clasificación:\")\n    print(classification_report(y_test, y_pred, target_names=['No Duplicada', 'Duplicada']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plotLossAccuracy(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluateModel(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}