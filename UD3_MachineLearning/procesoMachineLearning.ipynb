{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unidad 3. El proceso de trabajo en Machine Learning",
   "id": "976061e71cb42b55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Introducción \n",
    "<br><br>\n",
    "Cuando necesitamos desarrollar una aplicación o modelo de inteligencia artificial \n",
    "en el mundo real normalmente los datos que necesitamos para nuestro modelo\n",
    "no están directamente disponibles en un formato adecuado. A menudo, incluso, \n",
    "estos datos provienen de diferentes fuentes de datos. En cualquier caso, \n",
    "debemos disponer de algún mecanismo para recuperar esos datos, limpiar las \n",
    "partes que no nos interesen y dejar el resto estructurado en un formato adecuado \n",
    "para nuestro programa. Además, conviene conocer la composición de estos \n",
    "datos y sus características relevantes. Este proceso general se suele componer \n",
    "al menos de tres etapas, que no necesariamente se deben realizar en el orden \n",
    "que aquí se indica. <br><br>\n",
    "1. Limpieza de datos (data cleaning): consiste en procesar los datos para \n",
    "eliminar las partes que no interesen y dar un formato adecuado a las que \n",
    "sí. Por ejemplo, gestionar los valores nulos que pueda haber, o los valores \n",
    "anómalos, ver el tipo de dato adecuado para cada columna, etc. <br><br>\n",
    "2. Ingeniería de datos (data engineering): consiste en generar nuevos \n",
    "datos a partir de los que ya existen. Aquí entran aspectos como la \n",
    "selección de características relevantes, la conversión de variables \n",
    "categóricas en numéricas, o el escalado de los datos para que todos \n",
    "tengan el mismo peso o importancia. <br><br>\n",
    "3. Análisis exploratorio de datos (EDA, Exploratory Data Analysis), que \n",
    "consiste en analizar los datos de que disponemos buscando patrones o \n",
    "información relevante: medias, modas, tendencias, distribución de \n",
    "valores, etc. Para ello se suele hacer uso de las representaciones gráficas \n",
    "que ya hemos trabajado con Matplotlib o Seaborn. <br><br>\n",
    "La calidad o precisión de las decisiones que pueda tomar un programa dependen \n",
    "en gran parte de la calidad de los datos que le proporcionamos de entrada. En \n",
    "general, podemos considerar que los datos de que disponemos son de calidad \n",
    "si se ajustan a las operaciones que sobre ellos se realizan. Dicho de otro modo, \n",
    "no hay un estándar para medir esa calidad de los datos y, en general, dependerá \n",
    "de su adecuación al programa en cuestión. <br><br>\n",
    "En este documento veremos algunos ejemplos de técnicas de tratamiento y \n",
    "análisis de datos que podemos aplicar en nuestros desarrollos, siempre como \n",
    "paso previo a la ingesta de datos por parte de la aplicación. Hay que tener en \n",
    "cuenta que, en muchas ocasiones, este proceso de tratamiento y limpieza de \n",
    "datos puede llevar mucho más tiempo que lo que supone el desarrollo del modelo \n",
    "en sí. <br><br>\n"
   ],
   "id": "a8ac09d8a1dc96ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Presentación del caso de prueba\n",
    "Tomaremos como base un dataset que contiene información sobre datos de \n",
    "salud de distintos pacientes: género, edad, peso, colesterol... Pretendemos \n",
    "desarrollar un modelo que pueda predecir la presión sanguínea alta de un \n",
    "paciente (también llamada presión sistólica, almacenada en la columna ap_hi) a \n",
    "partir del resto de información. Como veremos, el documento CSV tiene muchas \n",
    "características, pero algunas de ellas van a resultar irrelevantes para determinar \n",
    "el valor objetivo, y otras que sí van a ser relevantes tendrán datos incompletos \n",
    "que tendremos que gestionar.\n",
    "Podemos crear un notebook de Jupyter y cargar el CSV para ir probando en él \n",
    "los pasos que explicaremos a continuación.\n"
   ],
   "id": "3b5b3e6f814e4a56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.1. Carga de librerías\n",
    "Comenzaremos importando las librerías necesarias para nuestro proyecto. \n",
    "Además de la librería sklearn incorporaremos algunos módulos que nos van a \n",
    "resultar de utilidad para ciertas tareas puntuales:"
   ],
   "id": "281bb0ecae29fa51"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:22.960417Z",
     "start_time": "2025-01-07T19:35:22.275568Z"
    }
   },
   "source": [
    "# Importar librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESP\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2. Carga del CSV y presentación inicial de los datos relevantes\n",
    "A continuación, cargaremos los datos del archivo CSV, mostraremos su \n",
    "estructura en pantalla y haremos un primer análisis del dato objetivo \n",
    "(columna ap_hi, presión sanguínea alta). Deberás poner el CSV en tu carpeta de \n",
    "trabajo."
   ],
   "id": "dfaa3cef6bdbf327"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:23.162500Z",
     "start_time": "2025-01-07T19:35:22.973516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar datos de CSV en variable \"datos\"\n",
    "datos = pd.read_csv('csvs/datos_salud2.csv')\n",
    "# Guardarnos en una variable el nombre de columna objetivo \"ap_hi\"\n",
    "valor_objetivo = 'ap_hi'\n",
    "# Mostrar 5 primeras filas\n",
    "print(datos.head())\n",
    "# Mostrar tamaño del dataset (filas y columnas)\n",
    "print(datos.shape)\n",
    "# Mostrar información relevante de ap_hi con su méthod describe\n",
    "print(datos[valor_objetivo].describe())\n"
   ],
   "id": "6232b449e1d260a4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_8928\\2582293461.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datos = pd.read_csv('csvs/datos_salud2.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   age gender  height  weight  ap_hi ap_lo        cholesterol    gluc  \\\n",
      "0   0  50.0      M   168.0    62.0    110    80             Normal  Normal   \n",
      "1   1  55.0      F   156.0    85.0    140    90  Well Above Normal  Normal   \n",
      "2   2  51.0      F   165.0    64.0    130    70  Well Above Normal  Normal   \n",
      "3   3  48.0      M   169.0    82.0    150   100             Normal  Normal   \n",
      "4   4  47.0      F   156.0    56.0    100    60             Normal  Normal   \n",
      "\n",
      "  smoke alco active cardio  \n",
      "0    No   No    Yes     No  \n",
      "1    No   No    Yes    Yes  \n",
      "2    No   No     No    Yes  \n",
      "3    No   No    Yes    Yes  \n",
      "4    No   No     No     No  \n",
      "(70000, 13)\n",
      "count    70000.000000\n",
      "mean       128.815429\n",
      "std        154.011614\n",
      "min       -150.000000\n",
      "25%        120.000000\n",
      "50%        120.000000\n",
      "75%        140.000000\n",
      "max      16020.000000\n",
      "Name: ap_hi, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Como podemos observar tenemos 70.000 registros, donde el valor máximo es \n",
    "16020, y el mínimo es -150, siendo el promedio 128.82 aproximadamente. \n",
    "Algunos de estos valores son muy anómalos, ya que una presión de 16020 es \n",
    "totalmente excesiva, y no se pueden tener presiones sanguíneas negativas. Más \n",
    "adelante veremos cómo gestionar estos valores.\n"
   ],
   "id": "86335b7c2c40388c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Limpieza de datos (data cleaning)\n",
    "Comenzaremos por el proceso de limpieza de datos, que consistirá \n",
    "fundamentalmente en detectar y corregir los valores nulos, las anomalías, y \n",
    "estudiar el tipo de dato más adecuado para cada columna. Estas correcciones \n",
    "son importantes debido al grave impacto que tiene para un modelo de IA la \n",
    "entrada de este tipo de errores."
   ],
   "id": "dfd25c59e83f809d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1. Gestión de valores nulos (missing values)\n",
    "En ocasiones es posible que algún campo o columna de nuestros datos tenga \n",
    "valores nulos o faltantes. Esto supone una pérdida de información, que se puede \n",
    "manifestar de distintas formas. Por ejemplo, si nos falta un valor numérico \n",
    "normalmente éste aparece como NaN (Not a Number) una sintaxis especial para \n",
    "identificar datos que se suponen numéricos pero no lo son. En otros casos \n",
    "podemos encontrarnos con un valor NA (Not Available) cuando ese dato en \n",
    "concreto no está disponible. <br><br>\n",
    "Por ejemplo, consideremos el siguiente listado de datos personales, donde falta \n",
    "la edad de una persona y el peso de otra:"
   ],
   "id": "95c8f3f108e2d8e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Nombre</th>\n",
    "    <th>Edad</th>\n",
    "    <th>Peso</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Juan</td>\n",
    "    <td>70</td>\n",
    "    <td>75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ana</td>\n",
    "    <td>NaN</td>\n",
    "    <td>70</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mario</td>\n",
    "    <td>30</td>\n",
    "    <td>NaN</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Laura</td>\n",
    "    <td>26</td>\n",
    "    <td>67</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "  \n"
   ],
   "id": "ecec398a4f10fdad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dependiendo del problema podemos optar por descartar la fila con valores \n",
    "omitidos, o por intentar reemplazar el valor omitido por otro simulado que no sea \n",
    "discordante con el resto, como por ejemplo la media, mediana o moda.\n"
   ],
   "id": "67a58d6bffcedfdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1.1. Detección o conteo de valores nulos\n",
    "La función isnull obtiene si una determinada casilla tiene o no valor. Podemos \n",
    "combinarla con la función sum para saber cuántas casillas vacías tenemos. \n",
    "Podemos construir un ejemplo como el anterior y ver cuántas casillas nulas hay:"
   ],
   "id": "1163ccb44eab1ebe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:23.484226Z",
     "start_time": "2025-01-07T19:35:23.470136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datosIniciales = {'Nombre': ['Juan', 'Ana', 'Mario', 'Laura'],\n",
    " 'Edad': [70, np.nan, 30, 26],\n",
    " 'Peso': [75, 70, np.nan, 67]}\n",
    "datos = pd.DataFrame(datosIniciales)\n",
    "print(\"Número de casillas nulas:\")\n",
    "print(datos.isnull().sum())\n",
    "# Dirá que la columna \"Nombre\" no tiene valores perdidos\n",
    "# y la columna \"Edad\" y \"Peso\" tienen un valor perdido cada una"
   ],
   "id": "c30cf69b703c9859",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de casillas nulas:\n",
      "Nombre    0\n",
      "Edad      1\n",
      "Peso      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Podríamos obtener, por ejemplo, todas las filas de un data frame que tengan nula \n",
    "una determinada casilla:\n"
   ],
   "id": "a69dd8cc72d563ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:23.528935Z",
     "start_time": "2025-01-07T19:35:23.519919Z"
    }
   },
   "cell_type": "code",
   "source": "nulos_edad = datos[datos['Edad'].isnull()]",
   "id": "150891b361e0931b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "El siguiente fragmento de código construye una tabla donde, para cada columna, \n",
    "muestra el porcentaje de casillas nulas que tiene respecto al total de filas:"
   ],
   "id": "f22a31ef6b264e75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:23.600691Z",
     "start_time": "2025-01-07T19:35:23.565801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "valores_nulos = datos.isnull().sum().sort_values(ascending=False)\n",
    "# Añadimos \"reset_index\" para numerar las filas resultantes\n",
    "ratio_nulos = (valores_nulos / len(datos)).reset_index()\n",
    "ratio_nulos.columns = ['Caracteristica', 'RatioNulos']\n",
    "ratio_nulos"
   ],
   "id": "389d996d06e55247",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Caracteristica  RatioNulos\n",
       "0           Edad        0.25\n",
       "1           Peso        0.25\n",
       "2         Nombre        0.00"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caracteristica</th>\n",
       "      <th>RatioNulos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edad</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peso</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nombre</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1.2. Opciones ante la presencia de valores nulos\n",
    "Si detectamos la presencia de valores nulos en una columna X, básicamente \n",
    "tenemos dos alternativas: <br><br>\n",
    "• Una consiste en reemplazar el valor nulo por un valor representativo, que \n",
    "puede ser por ejemplo la media o mediana de valores de la columna (en el \n",
    "caso de valores numéricos) o la moda (el valor que más se repite, útil para \n",
    "valores categóricos). También hay otras opciones, como reemplazar por el \n",
    "valor máximo o el mínimo de la columna. Es lo que se conoce \n",
    "como imputación de valores. Usaremos en cualquier caso el \n",
    "método fillna sobre la columna en cuestión, indicando el valor de \n",
    "reemplazo."
   ],
   "id": "12306a210a47653f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.349324Z",
     "start_time": "2025-01-07T19:35:23.728269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reemplazo por ceros\n",
    "datos['X'].fillna(0, inplace=True)\n",
    "# Reemplazo por la media\n",
    "datos['X'].fillna(datos['X'].mean(), inplace=True)\n",
    "# Reemplazo por la mediana\n",
    "datos['X'].fillna(datos['X'].median(), inplace=True)\n",
    "# Reemplazo por la moda (devuelve un array con las ocurrencias mayores)\n",
    "datos['X'].fillna(datos['X'].mode()[0], inplace=True)"
   ],
   "id": "47ee55e185e374c1",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'X'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'X'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Reemplazo por ceros\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mdatos\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;241m0\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Reemplazo por la media\u001B[39;00m\n\u001B[0;32m      4\u001B[0m datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfillna(datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmean(), inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3810\u001B[0m     ):\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'X'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Otra consiste en eliminar las filas o registros que contengan un valor nulo \n",
    "en esa columna. No es aconsejable en algunos casos porque pueden \n",
    "suponer mucha pérdida de información pero, si se opta por esta opción, \n",
    "podemos utilizar la función dropna de Pandas. Esta función altera el \n",
    "contenido de la colección sobre la que se aplica (además, debemos \n",
    "especificar el parámetro inplace=True, habitual en Pandas para actualizar \n",
    "la colección original)."
   ],
   "id": "46539c19eb767eb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.374659600Z",
     "start_time": "2024-12-19T16:42:45.889706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Borrar filas con algún campo nulo\n",
    "datos.dropna(inplace=True)\n",
    "# Borrar filas que tengan en la columna \"X\" un valor nulo\n",
    "datos.dropna(subset=['X'], inplace=True)"
   ],
   "id": "95257b59d2acc718",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1.3. Aplicación al ejemplo\n",
    "En lo que respecta a nuestro ejemplo, vamos a mostrar el porcentaje de nulos \n",
    "que hay en cada columna, volviendo a usar el código anterior. Vemos que las \n",
    "columnas con nulos son age, gender y cardio. En general son porcentajes muy \n",
    "bajos de nulos, y podríamos eliminar las filas afectadas con dropna, pero vamos \n",
    "a optar por diferentes alternativas <br><br>\n",
    "• Reemplazaremos los valores nulos de la edad (columna numérica) por la \n",
    "media de la columna <br><br>\n",
    "• Reemplazaremos los valores nulos del género (columna no numérica) por \n",
    "la moda de la columna <br><br>\n",
    "• Eliminaremos las filas que tengan el campo cardio nulo\n"
   ],
   "id": "3633e86135dc16a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.416145300Z",
     "start_time": "2024-12-19T16:43:48.293100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reemplazo de edades nulas por la media\n",
    "datos['age'] = datos['age'].fillna(datos['age'].mean())\n",
    "# Reemplazo de géneros nulos por la moda\n",
    "datos['gender'] = datos['gender'].fillna(datos['gender'].mode()[0])\n",
    "# Eliminación de cardios nulos\n",
    "datos = datos.dropna(subset=['cardio'])"
   ],
   "id": "ee585327db204f0c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2. Tipos de datos de las columnas\n",
    "Vamos a analizar ahora el tipo de datos de cada columna para ver si es el \n",
    "adecuado. Esto se puede ver fácilmente con la propiedad dtypes del dataset. En \n",
    "nuestro ejemplo podemos ver que las columnas numéricas tienen un tamaño de \n",
    "64 bits, cuando en realidad podrían ser de 32. También vemos que la \n",
    "columna ap_lo (presión sanguínea baja o diastólica) está marcada como \n",
    "tipo object, no numérica, lo que dificultará su uso en algunas operaciones. \n",
    "Cambiaremos su tipo también a int32:\n"
   ],
   "id": "2781dd993cf67409"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.437530700Z",
     "start_time": "2024-12-19T16:43:52.069799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datos = datos.astype({'age': 'int32', 'height': 'float32',\n",
    " 'weight': 'float32', 'ap_hi': 'int32', 'ap_lo': 'int32'})"
   ],
   "id": "ac54e1758652a09b",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Only a column name can be used for the key in a dtype mappings argument. 'age' not found in columns.\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9104\\714949179.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m datos = datos.astype({'age': 'int32', 'height': 'float32',\n\u001B[0m\u001B[0;32m      2\u001B[0m  \u001B[1;34m'weight'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'float32'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ap_hi'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'int32'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ap_lo'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'int32'\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, dtype, copy, errors)\u001B[0m\n\u001B[0;32m   6601\u001B[0m             \u001B[0mdtype_ser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSeries\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mobject\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6602\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6603\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mcol_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdtype_ser\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6604\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mcol_name\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6605\u001B[1;33m                     raise KeyError(\n\u001B[0m\u001B[0;32m   6606\u001B[0m                         \u001B[1;34m\"Only a column name can be used for the \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6607\u001B[0m                         \u001B[1;34m\"key in a dtype mappings argument. \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6608\u001B[0m                         \u001B[1;33mf\"\u001B[0m\u001B[1;33m'\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mcol_name\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m' not found in columns.\u001B[0m\u001B[1;33m\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Only a column name can be used for the key in a dtype mappings argument. 'age' not found in columns.\""
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<b>Nota</b><br><br>\n",
    "En el caso de que una columna tenga valores NA o inf es posible que no admita \n",
    "la conversión a tipo numérico. Habría que convertir o eliminar primero esos \n",
    "valores, para luego tratar el tipo de dato"
   ],
   "id": "4febd94b1992f66a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3. Gestión de anomalías (outliers)\n",
    "Podemos definir una anomalía como la observación de algún dato que difiere \n",
    "significativamente del resto. Lo que se conoce en términos técnicos como \n",
    "un outlier. Por ejemplo, la siguiente lista contiene medidas de la altura de una \n",
    "puerta realizadas por diferentes personas:\n"
   ],
   "id": "101994e69691e19a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`2.1m, 2.3m, 4.5m, 2.2m, 2.4m`<br><br>\n",
    "Claramente podemos detectar una anomalía en el tercer valor (4.5m) que difiere \n",
    "significativamente del resto. Los datos que extraemos del mundo real a menudo \n",
    "contienen este tipo de anomalías, y es importante detectarlas y gestionarlas \n",
    "aplicando alguna técnica conocida.\n"
   ],
   "id": "7149bafa67d541ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3.1. Detección gráfica de anomalías\n",
    "Una primera aproximación a la detección de anomalías (en columnas numéricas) \n",
    "puede ser un gráfico de cajas (box plot), que represente la distribución de los \n",
    "valores de esa columna. Los puntos que queden más allá de los bigotes de las \n",
    "cajas serán las anomalías destacadas de ese campo.\n",
    "Aplicado a nuestro ejemplo, podemos sacar el diagrama de cajas de las \n",
    "columnas <i>age, height, weight, ap_hi y ap_lo</i>:"
   ],
   "id": "39128d93e66a207f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "categorias = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8, 10))\n",
    "axes = axes.flatten()\n",
    "for i, var in enumerate(categorias):\n",
    " axes[i].boxplot(datos[var], tick_labels=[var])\n",
    " axes[i].set_title(f'Distribución de {var}')\n",
    " axes[i].set_ylabel('Valores')\n",
    "for j in range(len(categorias), len(axes)):\n",
    " axes[j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "420c88b91cd63d34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2. Detección de anomalías de forma matemática\n",
    "Ver las anomalías en un gráfico puede ayudar a hacernos una idea de qué \n",
    "valores son los que hay que controlar en una categoría. Sin embargo, para poder \n",
    "detectarlas en el código y poderlas eliminar/modificar es necesario disponer a \n",
    "veces de algún método matemático. En este apartado proponemos varias \n",
    "alternativas.<br><br>\n",
    "Una estrategia habitual es la detección de anomalías basada en la mediana, \n",
    "que considera la mediana de un conjunto de valores como punto de referencia. \n",
    "Esta mediana simplemente es el valor del elemento central de un conjunto \n",
    "ordenado de valores. A partir de esta mediana, se define un cierto umbral \n",
    "alrededor, y cualquier valor que exceda ese umbral se considera una anomalía. \n",
    "Por ejemplo, dado un conjunto de valores en la variable valores y un umbral de \n",
    "0.3, podríamos detectar qué valores son anómalos de este modo:"
   ],
   "id": "5260f6db4b42c4bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "valores = ... # Suponemos un conjunto de valores\n",
    "mediana = np.median(valores)\n",
    "umbral = 0.3\n",
    "outliers = []\n",
    "for elemento in valores:\n",
    " if abs(mediana - elemento) > umbral:\n",
    "    outliers.append(elemento)"
   ],
   "id": "24532c27d1aa1054"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Otra alternativa es la <b>detección de anomalías basada en la media</b>. Consiste \n",
    "en utilizar la media de valores como punto de referencia, junto con la desviación \n",
    "típica. Esto evita tener que definir umbrales arbitrarios como en el caso anterior. \n",
    "Simplemente descartamos o anotamos como anomalías todos aquellos valores \n",
    "que excedan el rango de la media más/menos la desviación típica. Así quedaría \n",
    "para el ejemplo anterior:\n"
   ],
   "id": "9a53b43a0a07747f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "media = np.mean(valores) \n",
    "desviacion = np.std(valores)\n",
    "outliers = []\n",
    "for elemento in valores:\n",
    " if media - desviacion > elemento or media + desviacion < elemento:\n",
    "    outliers.append(elemento)"
   ],
   "id": "1f010bbc5956c8c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En el caso de que la desviación típica sea un umbral insuficiente, podemos optar \n",
    "por una variante conocida como <b>puntuación Z</b>, que consiste en definir como \n",
    "umbral un múltiplo de la desviación típica (puntuación Z) que indica a cuántas \n",
    "desviaciones típicas está un valor con respecto a la media:\n"
   ],
   "id": "2b13c91d028561d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`Z = (valor - media) / desviación típica`\n",
   "id": "d6c479080912a828"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si la puntuación Z excede un cierto umbral razonable, consideramos al valor una \n",
    "anomalía. Aquí vemos cómo quedaría el ejemplo anterior usando una puntuación \n",
    "Z de 1.5:\n"
   ],
   "id": "89f3fbf31063b5cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "media = np.mean(valores) \n",
    "desviacion = np.std(valores) \n",
    "outliers = []\n",
    "for elemento in valores:\n",
    " z = abs(elemento - media) / desviacion\n",
    " if z > 1.5:\n",
    "    outliers.append(elemento)"
   ],
   "id": "75ce4e7767a5514c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finalmente, podemos optar por la técnica del cálculo del <b>rango \n",
    "intercuartílico</b> (IQR, InterQuartile Range). Un cuartil es una medida que divide \n",
    "el conjunto de valores en 4 áreas o intervalos. El rango intercuartil (IQR) es la \n",
    "zona que agrupa las dos áreas intermedias (segundo y tercer cuartil). <br><br>\n",
    "\n",
    "• El primer cuartil (Q1) deja a su izquierda el 25% de los valores<br>\n",
    "• El segundo cuartil (Q2) es siempre la mediana<br>\n",
    "• El tercer cuartil (Q3) deja a su izquierda el 75% de los valores<br><br>\n",
    "\n",
    "Según esta técnica, todos los valores que queden fuera de este rango IQR se \n",
    "consideran anomalías y deben descartarse. Podemos aplicar algún tipo de \n",
    "umbral para corregir esto e incluir valores que no se diferencien demasiado.<br><br>\n",
    "En Python, la función ``percentile`` de NumPy automáticamente ordena el vector \n",
    "de valores y calcula los cuartiles o percentiles que le digamos (es decir, los \n",
    "valores que suponen un X% del total de valores). Así quedaría el código en el \n",
    "ejemplo anterior:\n"
   ],
   "id": "539c93daa3a05db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Nos interesan los cuartiles del 25% y 75%\n",
    "Q1, Q3 = np.percentile(valores, [25, 75])\n",
    "IQR = Q3 - Q1\n",
    "outliers = []\n",
    "for elemento in valores:\n",
    " if elemento < (Q1 - 1.5 * IQR) or elemento > (Q3 + 1.5 * IQR):\n",
    "    outliers.append(elemento)"
   ],
   "id": "8111fcb09e01cd0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En este caso, hemos utilizado como umbral 1.5 veces el valor del IQR, \n",
    "descartando los valores que queden más allá, por arriba o por abajo.\n"
   ],
   "id": "b4d51b27e5c4c344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3.3. Aplicación al ejemplo\n",
    "¿Qué hacer con los datos que son outliers? Dependerá del problema en sí. \n",
    "Podemos descartarlos (quitarlos de la secuencia original de datos) o imputarlos \n",
    "(asignarles un valor alternativo, como por ejemplo el límite superior o inferior del \n",
    "umbral, entre otras opciones). En nuestro caso tomaremos las siguientes \n",
    "decisiones, en vista de los gráficos de caja obtenidos:<br><br>\n",
    "• Ignoraremos las anomalías de la edad, ya que corresponden a pacientes \n",
    "jóvenes (en torno a 30 años) pero podemos tenerlos en cuenta.<br><br>\n",
    "• Eliminaremos los registros de pacientes con altura superior a 230 cm<br><br>\n",
    "• Asignaremos un umbral inferior de 25 kg a todos los pacientes que pesen \n",
    "menos de esa cantidad (ignoraremos las anomalías de exceso de peso, \n",
    "porque pueden ser reales)<br><br>\n",
    "• Eliminaremos todas las presiones sanguíneas (ap_hi y ap_lo) que sean \n",
    "negativas o superiores a una puntuación Z de 2."
   ],
   "id": "83155d4bfa0c42f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Eliminar pacientes con altura superior a 230 cm\n",
    "datos = datos[datos['height'] <= 230]\n",
    "# Asignar 25 Kg a los pacientes que pesen menos de esa cantidad\n",
    "datos['weight'] = datos['weight'].apply(\n",
    " lambda x: 25 if x < 25 else x\n",
    ")\n",
    "# Eliminar presiones sanguíneas negativas\n",
    "datos = datos[(datos['ap_hi'] >= 0) & (datos['ap_lo'] >= 0)]\n",
    "# Eliminar presiones sanguíneas superiores a Z = 2\n",
    "# Paso 1: Obtenemos valores anómalos\n",
    "media1 = datos['ap_hi'].mean()\n",
    "media2 = datos['ap_lo'].mean()\n",
    "desv1 = datos['ap_hi'].std()\n",
    "desv2 = datos['ap_lo'].std()\n",
    "outliers1 = []\n",
    "outliers2 = []\n",
    "Z = 2\n",
    "for elemento in datos['ap_hi'].values:\n",
    " z = abs(elemento - media1) / desv1\n",
    " if z > Z:\n",
    "    outliers1.append(elemento)\n",
    "for elemento in datos['ap_lo'].values:\n",
    " z = abs(elemento - media2) / desv2\n",
    " if z > Z:\n",
    "    outliers2.append(elemento)\n",
    "# Paso 2: eliminar filas con outliers\n",
    "datos = datos[~datos['ap_hi'].isin(outliers1)]\n",
    "datos = datos[~datos['ap_lo'].isin(outliers2)]\n"
   ],
   "id": "8ea640ca1cb76ee8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si consultamos la propiedad shape del dataset tras estos pasos de limpieza \n",
    "podremos ver que se han eliminado poco más de 1.000 registros de los 70.000 \n",
    "que teníamos inicialmente, lo que es una cantidad aceptable.\n"
   ],
   "id": "a7918f31fba0cfae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Ingeniería de características (feature engineering)",
   "id": "14f09564caf63d9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora que ya hemos hecho una limpieza inicial de los datos de entrada, vamos \n",
    "a pasar al proceso de feature engineering. Entre otras cosas, este proceso \n",
    "consiste en: <br><br>\n",
    "• Codificar las variables categóricas (textuales) para darles un valor \n",
    "numérico, que es más apropiado para hacer operaciones matemáticas \n",
    "con estos valores de cara a obtener una predicción o resultado final<br>\n",
    "• Elegir qué características del conjunto de datos son más relevantes \n",
    "para el cálculo que queremos realizar. Es lo que se conoce como feature \n",
    "selection.<br>\n",
    "• Escalar los valores numéricos en un rango homogéneo, para que no haya \n",
    "valores con una magnitud superior a la de otros."
   ],
   "id": "79bde3d98ba61793"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1. Codificación de variables categóricas\n",
    "\n",
    "En muchos datasets podremos encontrar datos alfanuméricos, también \n",
    "llamados categorizados. Por ejemplo, nombres de ciudades, o calificaciones \n",
    "alfabéticas (\"buena\", \"muy buena\"...). Estos datos categorizados no suelen \n",
    "formar parte de algunos análisis por la imposibilidad de hacer operaciones con \n",
    "ellos: no podemos sacar la media de unas ciudades, por ejemplo, o multiplicar \n",
    "una valoración \"buena\" por un coeficiente numérico.\n",
    "Para evitar esto y hacer que esos datos también formen parte del problema a \n",
    "resolver, lo que se suele hacer es codificarlos en datos numéricos. Existen para \n",
    "ello distintas estrategias; comentaremos aquí un par de ellas: <br><br>\n",
    "• Codificación de etiquetas (label encoding)<br>\n",
    "• Codificación one hot"
   ],
   "id": "33275e609813a00d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1.1. Codificación de etiquetas (label encoding)\n",
    "El etiquetado de datos categóricos consiste en asignar un valor numérico a cada \n",
    "posible valor categórico de una serie de datos. Por ejemplo, imaginemos una \n",
    "tabla como la siguiente, que podría almacenar algunos datos de participantes de \n",
    "un club deportivo: <br><br>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Nacionalidad</th>\n",
    "    <th>Edad</th>\n",
    "    <th>Peso</th>\n",
    "    <th>Socio</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>España</td>\n",
    "    <td>34</td>\n",
    "    <td>88.4</td>\n",
    "    <td>Si</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Portugal</td>\n",
    "    <td>38</td>\n",
    "    <td>95.6</td>\n",
    "    <td>Si</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>España</td>\n",
    "    <td>30</td>\n",
    "    <td>90.2</td>\n",
    "    <td>No</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Francia</td>\n",
    "    <td>40</td>\n",
    "    <td>96.7</td>\n",
    "    <td>No</td>\n",
    "</tr>\n",
    "  <tr>\n",
    "    <td>Portugal</td>\n",
    "    <td>37</td>\n",
    "    <td>99.3</td>\n",
    "    <td>Si</td>\n",
    "</tr>\n",
    "  <tr>\n",
    "    <td>España</td>\n",
    "    <td>32</td>\n",
    "    <td>82.4</td>\n",
    "    <td>Si</td>  \n",
    "</tr>\n",
    "</table>\n",
    "<br>\n",
    "Podemos construir el dataframe equivalente de este modo:"
   ],
   "id": "99f4b03337042003"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.503323Z",
     "start_time": "2024-12-17T12:28:29.120092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datos = { 'Nacionalidad': ['España', 'Portugal', 'España',\n",
    " 'Francia', 'Portugal', 'España'],\n",
    " 'Edad': [34, 38, 30, 40, 37, 32],\n",
    " 'Peso': [88.4, 95.6, 90.2, 96.7, 99.3, 82.4],\n",
    " 'Socio': ['Si', 'Si', 'No', 'No', 'Si', 'Si']\n",
    " }\n",
    "df = pd.DataFrame(datos)"
   ],
   "id": "fdd92f7cdc8cc0bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si quisiéramos establecer una conexión entre los datos de las personas y si son \n",
    "socios o no, no podríamos hacer nada con la nacionalidad, porque es categórica. \n",
    "Tendríamos que asignarle un valor numérico equivalente para poder, por \n",
    "ejemplo, multiplicarla por un coeficiente en una ecuación y establecer así una \n",
    "correlación entre la nacionalidad y la probabilidad de ser socio o no. <br><br>\n",
    "Para ello, el primer paso que tendremos que dar es definir el tipo de dato de la(s) \n",
    "columna(s) afectada(s) como category en lugar del tipo por defecto object que \n",
    "les asigna Pandas:\n"
   ],
   "id": "9d878288464008cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.510372600Z",
     "start_time": "2024-12-17T12:35:59.798375Z"
    }
   },
   "cell_type": "code",
   "source": "df['Nacionalidad'] = df['Nacionalidad'].astype('category')\n",
   "id": "306366368abc3133",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Después, podemos obtener los códigos que automáticamente se han asignado \n",
    "a cada categoría con las propiedades cat.codes, e incluso crear otra columna con \n",
    "ello:"
   ],
   "id": "3e82203157ddedf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.513679700Z",
     "start_time": "2024-12-17T12:36:37.505958Z"
    }
   },
   "cell_type": "code",
   "source": "df['Cod_Nacionalidad'] = df['Nacionalidad'].cat.codes\n",
   "id": "e85c583fd36a96fd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "También podemos aplicar este tipo de codificación a la columna Socio, \n",
    "obteniendo los valores alternativos 0 y 1:"
   ],
   "id": "e13f5662f25a119c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.516949200Z",
     "start_time": "2024-12-17T12:37:55.799543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['Socio'] = df['Socio'].astype('category')\n",
    "df['Cod_Socio'] = df['Socio'].cat.codes\n",
    "df.head()"
   ],
   "id": "f82bc1152df41dc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Nacionalidad  Edad  Peso Socio  Cod_Nacionalidad  Cod_Socio\n",
       "0       España    34  88.4    Si                 0          1\n",
       "1     Portugal    38  95.6    Si                 2          1\n",
       "2       España    30  90.2    No                 0          0\n",
       "3      Francia    40  96.7    No                 1          0\n",
       "4     Portugal    37  99.3    Si                 2          1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nacionalidad</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Peso</th>\n",
       "      <th>Socio</th>\n",
       "      <th>Cod_Nacionalidad</th>\n",
       "      <th>Cod_Socio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>España</td>\n",
       "      <td>34</td>\n",
       "      <td>88.4</td>\n",
       "      <td>Si</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>38</td>\n",
       "      <td>95.6</td>\n",
       "      <td>Si</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>España</td>\n",
       "      <td>30</td>\n",
       "      <td>90.2</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francia</td>\n",
       "      <td>40</td>\n",
       "      <td>96.7</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>37</td>\n",
       "      <td>99.3</td>\n",
       "      <td>Si</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1.2. Codificación one hot\n",
    "La codificación de etiquetas anterior puede resultar problemática en algunas \n",
    "ocasiones. Al asignar un valor numérico diferente a cada categoría de un \n",
    "conjunto, sin quererlo, se establece un orden. Así, para el ejemplo anterior, si por \n",
    "ejemplo obtenemos que España tiene el valor 0, Portugal el 1 y Francia el 2, si usamos esas codificaciones en un sistema de machine learning se podría llegar \n",
    "a deducir que Francia es mayor o mejor que España porque 2 > 0. <br><br>\n",
    "Como ése no suele ser el propósito, una alternativa consiste en utilizar la \n",
    "codificación one hot. Una de las principales características de este tipo de \n",
    "codificación es que no puede haber ninguna relación de orden entre los \n",
    "elementos codificados, porque se codifican en distintas columnas, como \n",
    "veremos a continuación. <br><br>\n",
    "Para aplicar codificación one hot con Pandas sobre una columna determinada, \n",
    "podemos emplear el método get_dummies, indicando el data frame con los datos, \n",
    "las columnas que queremos codificar (en el parámetro columns, en forma de lista), \n",
    "y el prefijo que queremos darle a cada nueva columna que se genera. \n",
    "Apliquémoslo a la columna de nacionalidad anterior, de este modo:\n"
   ],
   "id": "b4790daf014cb753"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.521362300Z",
     "start_time": "2024-12-17T12:38:56.511764Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.get_dummies(df, columns=['Nacionalidad'], prefix='Nac')",
   "id": "dab9c35ae7a517c0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La función `get_dummies` devuelve un data frame donde se sustituyen las \n",
    "columnas indicadas por las nuevas. Esto elimina la columna \n",
    "categórica Nacionalidad y la reemplaza por las columnas one hot que se \n",
    "generan. Obtendremos este resultado:"
   ],
   "id": "ae561d4adc12e403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.524738200Z",
     "start_time": "2024-12-17T12:39:28.628261Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "bec0695b34e1f821",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Edad  Peso Socio  Cod_Nacionalidad  Cod_Socio  Nac_España  Nac_Francia  \\\n",
       "0    34  88.4    Si                 0          1        True        False   \n",
       "1    38  95.6    Si                 2          1       False        False   \n",
       "2    30  90.2    No                 0          0        True        False   \n",
       "3    40  96.7    No                 1          0       False         True   \n",
       "4    37  99.3    Si                 2          1       False        False   \n",
       "\n",
       "   Nac_Portugal  \n",
       "0         False  \n",
       "1          True  \n",
       "2         False  \n",
       "3         False  \n",
       "4          True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edad</th>\n",
       "      <th>Peso</th>\n",
       "      <th>Socio</th>\n",
       "      <th>Cod_Nacionalidad</th>\n",
       "      <th>Cod_Socio</th>\n",
       "      <th>Nac_España</th>\n",
       "      <th>Nac_Francia</th>\n",
       "      <th>Nac_Portugal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>88.4</td>\n",
       "      <td>Si</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>95.6</td>\n",
       "      <td>Si</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>90.2</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>96.7</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>99.3</td>\n",
       "      <td>Si</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si no queremos perder la columna categórica por algún motivo, podemos usar la \n",
    "función de este otro modo: lo que hacemos es generar las columnas one \n",
    "hot aparte (sólo pasándole la columna o columnas a codificar, en lugar de todo \n",
    "el data frame), y luego enlazarlas con join al data frame original:\n"
   ],
   "id": "ad5797907ab8aae2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.535459900Z",
     "start_time": "2024-12-17T12:40:09.298477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columnas_one_hot = pd.get_dummies(df['Nacionalidad'],\n",
    " columns=['Nacionalidad'], prefix='Nac')\n",
    "df = df.join(columnas_one_hot)\n"
   ],
   "id": "e6c35652dd2795d8",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Nacionalidad'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Nacionalidad'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m columnas_one_hot \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mNacionalidad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m,\n\u001B[0;32m      2\u001B[0m  columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNacionalidad\u001B[39m\u001B[38;5;124m'\u001B[39m], prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNac\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mjoin(columnas_one_hot)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3810\u001B[0m     ):\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Nacionalidad'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Como podemos ver, se generan tantas columnas como posibles valores tiene la \n",
    "categoría, y así se pone a 1 la columna a la que pertenece, y a 0 el resto.\n"
   ],
   "id": "8455f2eeac2a91e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1.3. Cuando usar cada tipo de codificación\n",
    "Existen otros tipos de codificación de datos categóricos que no hemos \n",
    "mencionado aquí, pero el uso de una u otra técnica obedecerá a ciertas premisas \n",
    "en el problema y los datos que estemos manejando. <br><br>\n",
    "En general, usaremos codificación one hot cuando no haya ninguna relación de \n",
    "orden entre las categorías, y usaremos label encoding cuando sí pueda haber \n",
    "cierta relación de orden o preferencia entre los valores categóricos. Para la \n",
    "columna Socio del caso anterior podríamos elegir cualquiera de las dos \n",
    "opciones, ya que sólo toma dos valores (0 o 1), e incluso podríamos querer decir \n",
    "que ser socio (1) es \"mejor\" que no serlo (0), según el problema. También \n",
    "usaremos label encoding para valoraciones, como en el siguiente ejemplo, \n",
    "donde sí interesa establecer un orden o gradación entre las categorías: <br><br>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Valoracion</th>\n",
    "    <th>Codificacion</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mala</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Regular</td>\n",
    "    <td>1</td>    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Buena</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Muy buena</td>\n",
    "    <td>3</td>    \n",
    "  </tr>\n",
    "</table>\n",
    "<br><br>\n",
    "Hay que tener en cuenta que, en este caso, los label encoders que hemos \n",
    "utilizado antes no tienen por qué asignar una numeración \"correcta\" para \n",
    "nuestros intereses, y tendríamos que hacerlo manualmente. Podemos crear una \n",
    "nueva columna paralela a la original Valoracion, llenarla con un valor inicial (cero, \n",
    "por ejemplo), y luego poner el valor a 1 en el caso de valoraciones Regular, 2 \n",
    "para valoraciones Buena, etc.\n",
    "\n",
    "\n"
   ],
   "id": "4b64448f2bc6f194"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.542550800Z",
     "start_time": "2024-12-17T12:45:43.692932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creamos columna Codificacion a partir de la Valoracion\n",
    "datos['Codificacion'] = datos['Valoracion'].replace(\n",
    " ['Mala', 'Regular', 'Buena', 'Muy buena'], [0, 1, 2, 3])"
   ],
   "id": "e0a58361b6f90750",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Valoracion'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Creamos columna Codificacion a partir de la Valoracion\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCodificacion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdatos\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mValoracion\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mreplace(\n\u001B[0;32m      3\u001B[0m  [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMala\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRegular\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBuena\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMuy buena\u001B[39m\u001B[38;5;124m'\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m])\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Valoracion'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1.4. Aplicación al ejemplo\n",
    "Volvamos a nuestro ejemplo de datos de salud. En el dataset tenemos varias \n",
    "columnas categóricas. Aquí indicaremos lo que vamos a hacer con ellas: <br><br>\n",
    "• La columna gender toma los valores M (masculino) o F (femenino). \n",
    "Crearemos una codificación one hot para distinguir con ceros y unos la \n",
    "pertenencia a uno u otro grupo. <br><br>\n",
    "• Las columnas cholesterol y gluc pueden tomar los valores Normal, Above \n",
    "Normal y Well Above Normal. En este caso sí nos interesa que haya una \n",
    "gradación o relación de orden, ya que un nivel de colesterol Normal es \n",
    "mejor que uno Above Normal, y éste a su vez es mejor que el Well Above \n",
    "Normal. Para estas columnas usaremos label encoding. <br><br>\n",
    "• Las columnas smoke, alco, active y cardio tienen valores Yes/No, que \n",
    "podemos codificar indistintamente como one hot o label encoding, ya que el resultado será el mismo (valores 1/0). Nos aseguraremos, en cualquier \n",
    "caso, que el valor Yes equivalga a 1 y el No a 0."
   ],
   "id": "876360263ce3ba1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.542550800Z",
     "start_time": "2024-12-17T12:47:24.338889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Codificación \"one hot\" de la columna \"gender\"\n",
    "datos = pd.get_dummies(datos, columns=['gender'], prefix='Gender')\n",
    "# Codificaciones \"label encoding\" de \"cholesterol\" y \"gluc\"\n",
    "datos['cholesterol'] = datos['cholesterol'].replace(['Normal', 'Above Normal',\n",
    "                                                     'Well Above Normal'], [0, 1, 2])\n",
    "datos['gluc'] = datos['gluc'].replace(['Normal', 'Above Normal',\n",
    "                                       'Well Above Normal'], [0, 1, 2])\n",
    "# Codificaciones binarias de columnas SI/NO\n",
    "datos['smoke'] = datos['smoke'].replace(['No', 'Yes'], [0, 1])\n",
    "datos['alco'] = datos['alco'].replace(['No', 'Yes'], [0, 1])\n",
    "datos['active'] = datos['active'].replace(['No', 'Yes'], [0, 1])\n",
    "datos['cardio'] = datos['cardio'].replace(['No', 'Yes'], [0, 1])\n",
    "# Vemos cómo queda el dataset\n",
    "print(datos.head())"
   ],
   "id": "bc7bfa64b33a7f52",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:460\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[1;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 460\u001B[0m     codes, categories \u001B[38;5;241m=\u001B[39m \u001B[43mfactorize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\algorithms.py:795\u001B[0m, in \u001B[0;36mfactorize\u001B[1;34m(values, sort, use_na_sentinel, size_hint)\u001B[0m\n\u001B[0;32m    793\u001B[0m             values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(null_mask, na_value, values)\n\u001B[1;32m--> 795\u001B[0m     codes, uniques \u001B[38;5;241m=\u001B[39m \u001B[43mfactorize_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_na_sentinel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m        \u001B[49m\u001B[43msize_hint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize_hint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sort \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\algorithms.py:595\u001B[0m, in \u001B[0;36mfactorize_array\u001B[1;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001B[0m\n\u001B[0;32m    594\u001B[0m table \u001B[38;5;241m=\u001B[39m hash_klass(size_hint \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(values))\n\u001B[1;32m--> 595\u001B[0m uniques, codes \u001B[38;5;241m=\u001B[39m \u001B[43mtable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfactorize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    597\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_sentinel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    598\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    599\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_na\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001B[39;00m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7281\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7195\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: unhashable type: 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Codificación \"one hot\" de la columna \"gender\"\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m datos \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dummies\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdatos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgender\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mGender\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Codificaciones \"label encoding\" de \"cholesterol\" y \"gluc\"\u001B[39;00m\n\u001B[0;32m      4\u001B[0m datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcholesterol\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcholesterol\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreplace([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNormal\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAbove Normal\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      5\u001B[0m  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWell Above Normal\u001B[39m\u001B[38;5;124m'\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m])\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:226\u001B[0m, in \u001B[0;36mget_dummies\u001B[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001B[0m\n\u001B[0;32m    224\u001B[0m     result \u001B[38;5;241m=\u001B[39m concat(with_dummies, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    225\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 226\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_get_dummies_1d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefix_sep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdummy_na\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    232\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdrop_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    233\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:250\u001B[0m, in \u001B[0;36m_get_dummies_1d\u001B[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m concat\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# Series avoids inconsistent NaN handling\u001B[39;00m\n\u001B[1;32m--> 250\u001B[0m codes, levels \u001B[38;5;241m=\u001B[39m \u001B[43mfactorize_from_iterable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(data, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    253\u001B[0m     input_dtype \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdtype\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3042\u001B[0m, in \u001B[0;36mfactorize_from_iterable\u001B[1;34m(values)\u001B[0m\n\u001B[0;32m   3037\u001B[0m     codes \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mcodes\n\u001B[0;32m   3038\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3039\u001B[0m     \u001B[38;5;66;03m# The value of ordered is irrelevant since we don't use cat as such,\u001B[39;00m\n\u001B[0;32m   3040\u001B[0m     \u001B[38;5;66;03m# but only the resulting categories, the order of which is independent\u001B[39;00m\n\u001B[0;32m   3041\u001B[0m     \u001B[38;5;66;03m# from ordered. Set ordered to False as default. See GH #15457\u001B[39;00m\n\u001B[1;32m-> 3042\u001B[0m     cat \u001B[38;5;241m=\u001B[39m \u001B[43mCategorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mordered\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   3043\u001B[0m     categories \u001B[38;5;241m=\u001B[39m cat\u001B[38;5;241m.\u001B[39mcategories\n\u001B[0;32m   3044\u001B[0m     codes \u001B[38;5;241m=\u001B[39m cat\u001B[38;5;241m.\u001B[39mcodes\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:462\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[1;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001B[0m\n\u001B[0;32m    460\u001B[0m     codes, categories \u001B[38;5;241m=\u001B[39m factorize(values, sort\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m--> 462\u001B[0m     codes, categories \u001B[38;5;241m=\u001B[39m \u001B[43mfactorize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    463\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype\u001B[38;5;241m.\u001B[39mordered:\n\u001B[0;32m    464\u001B[0m         \u001B[38;5;66;03m# raise, as we don't have a sortable data structure and so\u001B[39;00m\n\u001B[0;32m    465\u001B[0m         \u001B[38;5;66;03m# the user should give us one by specifying categories\u001B[39;00m\n\u001B[0;32m    466\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    467\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not ordered, please \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    468\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexplicitly specify the categories order \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    469\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby passing in a categories argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    470\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\algorithms.py:795\u001B[0m, in \u001B[0;36mfactorize\u001B[1;34m(values, sort, use_na_sentinel, size_hint)\u001B[0m\n\u001B[0;32m    792\u001B[0m             \u001B[38;5;66;03m# Don't modify (potentially user-provided) array\u001B[39;00m\n\u001B[0;32m    793\u001B[0m             values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(null_mask, na_value, values)\n\u001B[1;32m--> 795\u001B[0m     codes, uniques \u001B[38;5;241m=\u001B[39m \u001B[43mfactorize_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_na_sentinel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m        \u001B[49m\u001B[43msize_hint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize_hint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sort \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    802\u001B[0m     uniques, codes \u001B[38;5;241m=\u001B[39m safe_sort(\n\u001B[0;32m    803\u001B[0m         uniques,\n\u001B[0;32m    804\u001B[0m         codes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m         verify\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    808\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\algorithms.py:595\u001B[0m, in \u001B[0;36mfactorize_array\u001B[1;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001B[0m\n\u001B[0;32m    592\u001B[0m hash_klass, values \u001B[38;5;241m=\u001B[39m _get_hashtable_algo(values)\n\u001B[0;32m    594\u001B[0m table \u001B[38;5;241m=\u001B[39m hash_klass(size_hint \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(values))\n\u001B[1;32m--> 595\u001B[0m uniques, codes \u001B[38;5;241m=\u001B[39m \u001B[43mtable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfactorize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    597\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_sentinel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    598\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    599\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_na\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001B[39;00m\n\u001B[0;32m    604\u001B[0m uniques \u001B[38;5;241m=\u001B[39m _reconstruct_data(uniques, original\u001B[38;5;241m.\u001B[39mdtype, original)\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7281\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7195\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En algunas ocasiones, la codificación one hot genera información redundante. \n",
    "En nuestro caso, tenemos ahora dos columnas Gender_F y Gender_M que nos \n",
    "indican si un paciente es hombre o mujer. Nos bastaría con una de las dos, ya \n",
    "que si no pertenece a un género automáticamente se le asigna el otro. Así que \n",
    "eliminamos, por ejemplo, la columna Gender_M del estudio."
   ],
   "id": "1a198b7058dc3f0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "datos.drop('Gender_M', axis=1, inplace=True)\n",
   "id": "3b8d9725defd6793"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2. Selección de características (feature selection)\n",
    "Vamos a abordar en este apartado la selección de las características o columnas \n",
    "más relevantes para nuestro estudio. Saber elegir las características correctas o \n",
    "adecuadas para un problema puede repercutir en múltiples ventajas, tales como \n",
    "la reducción del proceso de aprendizaje, el ahorro de memoria de \n",
    "almacenamiento o la simplificación del modelo a considerar. Además, ayuda a \n",
    "evitar la codependencia o colinearidad entre atributos. Es decir, si un atributo \n",
    "depende fuertemente de otro, quizá analizando uno de los dos sea suficiente, y \n",
    "podemos descartar el segundo. <br><br>\n",
    "Existen distintas estrategias que nos pueden ayudar a tomar esta decisión. Por \n",
    "ejemplo, podemos entrenar a nuestro sistema con diferentes combinaciones de \n",
    "características, y ver en cuáles se obtienen resultados significativamente \n",
    "diferentes. En nuestro caso vamos a optar por analizar la correlación entre los \n",
    "valores, a través de un mapa de calor de correlación (correlation heatmap) \n",
    "gracias a Pandas y Seaborn.<br><br>\n",
    "Este mapa nos mostrará de forma gráfica las dependencias entre las diferentes \n",
    "variables involucradas: una correlación alta (cercana a 1) entre dos variables \n",
    "indicará que cuando una aumenta la otra también lo hace, y esto puede significar \n",
    "que una influye mucho en el valor de la otra (y es determinante para calcularlo), \n",
    "o bien que las dos tienen un comportamiento similar (y podemos prescindir de \n",
    "una de ellas). También tenemos que tener en cuenta la correlación inversa \n",
    "(cercana a -1) y ver qué hacer en esos casos. <br><br>\n",
    "En definitiva, podemos eliminar características del conjunto por dos motivos: <br><br>\n",
    "• Porque sean redundantes (muy correlacionadas con otras que ya \n",
    "vamos a incluir) <br><br>\n",
    "• Porque sean irrelevantes (no afecten al resultado que se quiere \n",
    "obtener) <br><br>\n",
    "Construimos un mapa de calor de correlación sobre nuestro dataset, de este \n",
    "modo:"
   ],
   "id": "8a45ed67c5621590"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(datos.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlación datos salud')\n",
    "plt.show()\n"
   ],
   "id": "b3e02eb357efc1e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En base a este mapa de correlación, podemos ir a la columna ap_hi, que es la \n",
    "que corresponde a nuestra variable objetivo, y ver qué otras variables son las \n",
    "que más influyen en su valor. Podemos ver que son la presión sanguínea baja \n",
    "(ap_lo), si padece o no problemas cardíacos (columna cardio), el peso (weight), \n",
    "la edad (age) y si tiene o no colesterol. El resto de valores son muy cercanos a \n",
    "0, es decir, no parecen muy relacionados con la presión sanguínea alta y se \n",
    "pueden eliminar. <br><br>\n",
    "Además, si observamos las variables que hemos seleccionado, es decir, \n",
    "ap_lo, cardio, weight, age y cholesterol podremos ver que no existe una \n",
    "correlación fuerte, lo que nos permitiría eliminar alguna de ellas y quedarnos con \n",
    "un conjunto más reducido. <br><br>\n",
    "Vamos entonces a quedarnos únicamente con estas columnas de \n",
    "nuestro dataset:\n"
   ],
   "id": "feeb42a8b4e50c3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "columnas_relevantes = ['age', 'weight', 'ap_lo', 'cholesterol', 'cardio']\n",
    "datos = datos[columnas_relevantes + [valor_objetivo]]\n",
    "datos.head()"
   ],
   "id": "190de88e29c42acc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3. Escalado de características (feature scaling)\n",
    "El escalado de características o feature scaling es una herramienta muy habitual \n",
    "en el procesamiento de datos. Consiste en dejar un conjunto de valores en un \n",
    "rango común o delimitado. Esto es bastante útil, por ejemplo, en el campo de las \n",
    "redes neuronales. Imaginemos una red que toma imágenes como datos de \n",
    "entrada. Los valores de los colores de esas imágenes pueden estar definidos en \n",
    "distintos rangos, según el formato de la imagen (0 a 255 para imágenes en \n",
    "escala de grises, o valores mayores para algunos modelos de color). En este \n",
    "caso, podría interesar que los valores de los colores de los píxeles estuvieran \n",
    "normalizados en un rango de 0 a 1 para un mejor tratamiento. <br><br>\n",
    "Otro ejemplo quizá más ilustrativo. Imaginemos que tenemos datos de clientes \n",
    "de una entidad bancaria. Guardamos su edad, sus ingresos anuales, provincia, \n",
    "etc: <br><br>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Edad</th>\n",
    "    <th>Ingresos</th>\n",
    "    <th>Provincia</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>34</td>\n",
    "    <td>28000</td>\n",
    "    <td>Sevilla</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>40</td>\n",
    "    <td>30000</td>\n",
    "    <td>Toledo</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>54</td>\n",
    "    <td>32000</td>\n",
    "    <td>Oviedo</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>43</td>\n",
    "    <td>38000</td>\n",
    "    <td>Badajoz</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br><br>\n",
    "Si queremos determinar cómo de parecidos o diferentes son dos individuos en \n",
    "base a ciertos datos de entrada (por ejemplo, edad y e ingresos), esto se puede \n",
    "conseguir calculando la \"distancia\" entre estos dos individuos en base a esos \n",
    "parámetros: \n",
    "<br><br>\n",
    "\n",
    "`raiz_cuadrada((edad1 - edad2)^2 + (ingresos1 - ingresos2)^2)`\n",
    "<br><br>\n",
    "El problema aquí lo encontramos en que ingresos y edad se mueven en rangos \n",
    "de valores diferentes, por lo que a la hora de determinar la diferencia entre dos \n",
    "individuos van a ser mucho más determinantes los ingresos, por ser valores \n",
    "mucho más altos. Así, una diferencia de ingresos de apenas 500 euros va a \n",
    "marcar mucho más la diferencia entre individuos que una diferencia de edad de \n",
    "30 años. Para evitar este problema, podemos escalar los datos a un rango \n",
    "común (por ejemplo, que ambas columnas se muevan en un rango de 0 a 1). \n",
    "Vamos a ver cómo se hace.\n"
   ],
   "id": "55ad2e7e40f587d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.3.1. Escalado por normalización\n",
    "Una primera estrategia de escalado es la normalización, también \n",
    "llamada <b>escalado min-max</b>, que deja todos los valores del conjunto en el rango \n",
    "de 0 a 1, aplicando la siguiente fórmula a cada valor: <br><br>\n",
    "\n",
    "`val_normalizado = (val_actual - val_min) / (val_max - val_min)`\n",
    "\n",
    "<br>\n",
    "\n",
    "Imaginemos un conjunto de datos como este:\n"
   ],
   "id": "fbe920c582079e8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "datosIniciales = {'Nombre': ['Juan', 'Ana', 'Mario', 'Laura'],\n",
    " 'Edad': [70, 40, 30, 26],\n",
    " 'Sueldo': [2800, 1200, 1750, 1420]}\n",
    "datos = pd.DataFrame(datosIniciales)"
   ],
   "id": "62919e4fa835acb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Podemos aplicar el escalado min-max a la colección, aplicándolo a cada \n",
    "columna afectada:\n"
   ],
   "id": "ebfbaa62e11d1287"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "datos['Edad'] = (datos['Edad'] - datos['Edad'].min()) / \\\n",
    " (datos['Edad'].max() - datos['Edad'].min())\n",
    "datos['Sueldo'] = (datos['Sueldo'] - datos['Sueldo'].min()) / \\\n",
    " (datos['Sueldo'].max() - datos['Sueldo'].min())"
   ],
   "id": "e25fa813679b7856"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En el caso de que todas las columnas de la colección sean numéricas y \n",
    "queramos normalizarlas, podemos aplicar una sola fórmula a todo el data \n",
    "frame (no es el caso de este ejemplo): <br><br>\n",
    "\n",
    "`datos = (datos - datos.min()) / (datos.max() - datos.min())\n",
    "`"
   ],
   "id": "587ae0576859b384"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.3.2. Escalado por estandarización\n",
    "Una segunda alternativa consiste en calcular la media y desviación típica del \n",
    "conjunto de datos a tratar. El valor normalizado se calcula entonces como: <br><br>\n",
    "`val_normalizado = (val_actual - media) / desviación` \n",
    "<br><br>\n",
    "En este caso, el rango de valores ya no va de 0 a 1, y admite valores negativos, \n",
    "pero al menos está acotado a un rango más controlado. Así quedaría en nuestro \n",
    "ejemplo anterior:\n"
   ],
   "id": "150e5af72112a136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "datos['Edad'] = (datos['Edad'] - datos['Edad'].mean()) / \\\n",
    " datos['Edad'].std()\n",
    "datos['Sueldo'] = (datos['Sueldo'] - datos['Sueldo'].mean()) / \\\n",
    " datos['Sueldo'].std()"
   ],
   "id": "bf97a140976774d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.3.3. Aplicación al ejemplo\n",
    "Vamos a aplicar el escalado a nuestro ejemplo. Las columnas codificadas \n",
    "como one hot no es necesario escalarlas, puesto que ya están acotadas en un \n",
    "rango 0-1. Las columnas codificadas como label encoding podríamos escalarlas \n",
    "si quisiéramos en el caso de que las etiquetas tuvieran valores más altos. Pero \n",
    "en nuestro caso sólo van del 0 al 2. Así que nos centraremos en la edad, el peso \n",
    "y la presión sanguínea baja.<br><br>\n",
    "Aplicaremos el escalado por estandarización.\n"
   ],
   "id": "d59a97dd3463741"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Escalamos la edad\n",
    "datos['age'] = (datos['age'] - datos['age'].mean()) / datos['age'].std()\n",
    "# Escalamos el peso\n",
    "datos['weight'] = (datos['weight'] - datos['weight'].mean()) \n",
    "datos['weight'].std()\n",
    "# Escalamos la presión baja\n",
    "datos['ap_lo'] = (datos['ap_lo'] - datos['ap_lo'].mean()) \n",
    "datos['ap_lo'].std()\n"
   ],
   "id": "7ebb73dc1f1cd4b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Análisis exploratorio de datos\n",
    "El análisis exploratorio de datos (EDA) es una etapa que suele darse \n",
    "simultáneamente a las dos anteriores, y nos permite sacar ciertas conclusiones \n",
    "sobre los datos con los que estamos trabajando. <br><br>\n",
    "De hecho, ya hemos hecho uso de ella cuando hemos mostrado los diagramas \n",
    "de cajas de las columnas numéricas para conocer la distribución de valores y ver \n",
    "posibles anomalías, y también cuando hemos obtenido el mapa de calor de \n",
    "correlación para conocer qué parámetros eran más relevantes para definir el \n",
    "valor objetivo. También cuando hemos empleado el método describe para \n",
    "obtener los datos estadísticos de la columna objetivo ap_hi, y conocer sus \n",
    "valores máximos, mínimos, media, etc. <br><br>\n",
    "También podemos obtener otras representaciones numéricas y gráficas, como \n",
    "por ejemplo un histograma con la distribución de valores de la columna \n",
    "objetivo ap_hi, o también gráficos de dispersión (scatter plots) que muestren la \n",
    "dependencia de cada columna relevante con la variable objetivo. Podemos \n",
    "mostrar todo esto en una matriz de 3 filas y 2 columnas, por ejemplo: <br><br>\n"
   ],
   "id": "b7b84345fcbdedec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))\n",
    "for i in range(0, len(columnas_relevantes)):\n",
    " fila = i // 2\n",
    " columna = i % 2\n",
    " ax[fila, columna].scatter(x = datos[columnas_relevantes[i]],\n",
    " y = datos[valor_objetivo])\n",
    " ax[fila, columna].set_title(columnas_relevantes[i] + \" frente a \" +\n",
    "valor_objetivo)\n",
    "ax[2, 1].hist(datos[valor_objetivo], bins=10)\n",
    "ax[2, 1].set_title(\"Distribución de valores de \" + valor_objetivo)\n"
   ],
   "id": "82912eb096cd7024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Desarrollo de un modelo simple\n",
    "Para terminar con este ejemplo vamos a construir un modelo de árbol de decisión \n",
    "que entrene con el conjunto de datos que hemos definido, y mediremos su \n",
    "precisión final (accuracy).\n"
   ],
   "id": "e4ddc95b08f17a19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1. Definición de la métrica\n",
    "En primer lugar, vamos a definir la métrica, es decir, una función que determine \n",
    "cómo de bien o mal estamos haciendo nuestra tarea (estimar la presión sistólica). \n",
    "Emplearemos para ello como medida el error absoluto medio (MAE)."
   ],
   "id": "edf230cd4c35a49b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def metrica(valores_reales, valores_predichos):\n",
    " return mean_absolute_error(valores_reales, valores_predichos)"
   ],
   "id": "cd1e385ff2f0d6f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En este caso el MAE puede resultar adecuado para obtener el error cometido, y \n",
    "nos devolverá cómo nos equivocamos (en promedio) con las estimaciones \n",
    "respecto a los valores reales, calculando la diferencia en valor absoluto entre \n",
    "unas y otras, y obteniendo la media de esos errores"
   ],
   "id": "cd9a357a97877657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2. Definición de conjuntos de entrenamiento y test\n",
    "En todo proceso de estimación es necesario disponer de un conjunto de datos \n",
    "con los que \"practicar\" y entrenar al sistema, y otro conjunto de datos, con \n",
    "resultados conocidos, con los que determinar si el sistema realmente ha \n",
    "aprendido a hacer las cosas adecuadamente o no. Este segundo conjunto se \n",
    "suele denominar datos de validación.<br><br>\n",
    "Usaremos la función `train_test_split` de sklearn para dividir automáticamente \n",
    "nuestro conjunto original de datos en dos partes: una para entrenamiento y otra \n",
    "para validación."
   ],
   "id": "bcd7fdb54939b6e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Indicamos que el 20% de los datos son para test\n",
    "# random_state sirve para que siempre se elijan los mismos datos para test.\n",
    "df_train, df_val = train_test_split(datos, test_size=0.2, random_state=12)\n"
   ],
   "id": "8770cbd04c5d689"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3. Definición del modelo\n",
    "Vamos ahora a definir un árbol de decisión simple para evaluar cómo de bien o \n",
    "mal estima esta presión sistólica con estos datos de entrenamiento y test:\n"
   ],
   "id": "52eed7b1cbbd9d1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "modelo_arbol = DecisionTreeRegressor(random_state=12, max_depth=7, \n",
    " min_samples_split=10)\n",
    "modelo_arbol.fit(df_train[columnas_relevantes], df_train[valor_objetivo])\n",
    "predicciones = modelo_arbol.predict(df_val[columnas_relevantes])\n",
    "error_val = metrica(df_val[valor_objetivo], predicciones)\n",
    "print(f'Métrica para datos de validación: {error_val}')\n"
   ],
   "id": "910bc2c470aafefe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si hemos seguido los pasos de este documento probablemente obtengamos un \n",
    "error promedio en torno a 7.6, es decir, 7 puntos de diferencia entre la presión \n",
    "predicha y la real. Es algo mejorable, pero nos da una idea, en definitiva, de los \n",
    "pasos que se deben seguir para preparar un conjunto de datos de cara a la \n",
    "definición de un modelo."
   ],
   "id": "b932d7f9b7fccc8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.4. Algunas consideraciones finales\n",
    "Hemos visto durante estos últimos apartados algunas estrategias de escalado y \n",
    "codificación de datos. Nos puede surgir la duda de cuándo aplicar unas u otras, \n",
    "y sobre qué datos de entrada en concreto. Realmente no hay una respuesta \n",
    "universal a esta pregunta, porque va a depender mucho del conjunto de datos \n",
    "de entrada y de lo que queramos hacer con ellos (regresión, clasificación, etc). \n",
    "Pero podemos dar algunas pautas generales a tener en cuenta: <br><br>\n",
    "• Normalmente escalaremos columnas que tengan rangos de valores muy \n",
    "dispares entre sí, para aunarlas todas en un rango común (usando \n",
    "normalización o estandarización, como queramos) <br><br>\n",
    "• Los valores one hot pueden escalarse o no, hay razones buenas en \n",
    "ambos casos, pero hay que tener en cuenta que, si los escalamos, \n",
    "perderemos esa noción de \"pertenencia a una categoría\" que nos dan los \n",
    "ceros y unos de cada columna, ya que pasarán a tener otros valores. Es \n",
    "habitual, por tanto, no escalarlos en muchos problemas. <br><br>\n",
    "• Los valores a predecir (típicamente conocidos como columna y o variable \n",
    "dependiente) también podemos decidir si escalarlos o no, dependiendo \n",
    "del problema: si estamos en un problema de clasificación, lo normal será \n",
    "no escalarlos, para que se asigne cada registro a una categoría igual a las \n",
    "que teníamos de entrada. Si estamos en un problema de regresión y \n",
    "hemos escalado los parámetros de entrada, es posible que también nos \n",
    "interese escalar el valor de salida (eje vertical)"
   ],
   "id": "b0cb62ce687de4bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Métricas de validación de modelos de regresión\n",
    "Además del MAE, usado anteriormente en el ejemplo, para valorar la calidad de\n",
    "los modelos de regresión tenemos otras métricas que podemos considerar:<br><br>\n",
    "<b>Error Cuadrático Medio (MSE)</b><br><br>\n",
    "Penaliza los errores más grandes más que el MAE, ya que los eleva al cuadrado.\n",
    "Esto puede ser útil si los errores grandes son especialmente importantes en tu\n",
    "problema. <br><br>\n",
    "<img src=\"img/captura2.png\" alt=\"captura2\"> <br><br>\n",
    "\n",
    "<b>Raíz del Error Cuadrático Medio (RMSE)</b><br><br>\n",
    "Es la raíz cuadrada del MSE, por lo que devuelve los errores en las mismas\n",
    "unidades que la variable objetivo.<br><br>\n",
    "<b>Coeficiente de Determinación (R²)</b><br><br>\n",
    "\n",
    "Mide qué tan bien las predicciones se ajustan a los datos reales. Va de 0 a 1 (en\n",
    "casos ideales) y puede ser negativo si el modelo es peor que simplemente usar\n",
    "la media de los datos.\n",
    "Veamos cómo invocarlos:"
   ],
   "id": "12f36c9cabdc944d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)"
   ],
   "id": "d678922d3be96463"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Modelos supervisados\n",
    "Modelos de Regresión Lineal <br><br>\n",
    "Regresión Lineal Múltiple: Este modelo podría ser útil para establecer una línea\n",
    "base y entender las relaciones lineales entre las variables."
   ],
   "id": "87c048b746f7fa3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ],
   "id": "721ecac03231913c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modelos Basados en Árboles <br><br>\n",
    "Random Forest Regressor: Este modelo ensemble podría mejorar la precisión\n",
    "de las predicciones al combinar múltiples árboles de decisión.\n"
   ],
   "id": "1c0a82b0075cd261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ],
   "id": "15f8eb32ba94d024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modelos de Boosting <br><br>\n",
    "Gradient Boosting Regressor: Podría capturar relaciones no lineales complejas\n",
    "en los datos de salud."
   ],
   "id": "70f57337bc97253d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n"
   ],
   "id": "326202c095e4972a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modelos de Vectores de Soporte <br><br>\n",
    "Support Vector Regression (SVR): Útil para capturar relaciones no lineales en\n",
    "los datos de presión sanguínea."
   ],
   "id": "3c50e49182ab86ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = SVR(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ],
   "id": "beeb2fd5dc773d04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Estos modelos podrían proporcionar diferentes perspectivas sobre la relación\n",
    "entre las características de salud y la presión sanguínea alta, permitiendo\n",
    "comparar su rendimiento con el DecisionTreeRegressor inicial\n"
   ],
   "id": "45cf3779e8123519"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Modelos no supervisados\n",
    "Clustering <br>\n",
    "K-means: Este algoritmo podría agrupar pacientes con características similares,\n",
    "lo que permitiría:<br>\n",
    "Identificar perfiles de riesgo para la presión arterial alta.<br>\n",
    "Descubrir patrones ocultos en los datos de salud.<br>\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Útil para:<br>\n",
    "Detectar grupos de pacientes con formas irregulares.<br>\n",
    "Identificar outliers en los datos de salud.<br>\n",
    "Reducción de Dimensionalidad<br>\n",
    "PCA (Principal Component Analysis): Ayudaría a:<br>\n",
    "Reducir la dimensionalidad del dataset, facilitando la visualización.<br>\n",
    "Identificar las características más importantes que influyen en la presión arterial.<br>\n",
    "Detección de Anomalías<br>\n",
    "Isolation Forest: Sería beneficioso para:<br>\n",
    "Detectar pacientes con valores atípicos en sus datos de salud.<br>\n",
    "Identificar posibles errores en la recolección de datos o casos médicos inusuales.<br><br>\n",
    "Estos modelos no supervisados complementarían el análisis supervisado,\n",
    "proporcionando una comprensión más profunda de la estructura de los datos y\n",
    "ayudando a identificar patrones que podrían no ser evidentes en el enfoque\n",
    "supervisado inicial.\n"
   ],
   "id": "91c8a474ea16292c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Mejora de los modelos\n",
    "Hay situaciones que complican la interpretación de los resultados de un modelo,\n",
    "la calidad del mismo, el rendimiento y muchas otras problemáticas. Para luchar\n",
    "con todo ello existen una serie de técnicas importantes a conocer. Veamos las\n",
    "más significativas. <br><br>\n",
    "### 9.1. Datasets desbalanceados: sobremuestreo y submuestreo\n",
    "Un dataset desbalanceado puede causar varios problemas en el desarrollo de\n",
    "modelos de machine learning. Los algoritmos suelen estar diseñados para\n",
    "maximizar la precisión global, lo que puede llevar a un sesgo hacia la clase\n",
    "mayoritaria, ignorando la clase minoritaria y reduciendo su capacidad para\n",
    "identificarla correctamente.<br><br>\n",
    "Esto genera métricas engañosas, como una precisión aparentemente alta, pero\n",
    "con un bajo desempeño en métricas clave como el recall o el F1-score de la\n",
    "clase minoritaria. Además, aumenta el riesgo de falta de generalización, ya que\n",
    "el modelo no aprende patrones útiles para la clase minoritaria, y en casos\n",
    "extremos puede provocar overfitting a la clase mayoritaria. Estos problemas\n",
    "tienen un impacto ético y práctico, especialmente en aplicaciones críticas como\n",
    "detección de fraudes o diagnósticos médicos, donde fallar en la clase minoritaria\n",
    "puede tener consecuencias significativas.<br><br>\n",
    "Por ejemplo, supongamos que tienes un dataset con transacciones bancarias, donde:<br><br>\n",
    "• 98% de las transacciones son legítimas (Clase 0).<br><br>\n",
    "• 2% son fraudulentas (Clase 1).<br><br>\n",
    "El problema es que el modelo podría aprender a predecir siempre Clase 0\n",
    "(legítima), logrando una precisión del 98%, debido a la gran cantidad de\n",
    "muestras de esa clase frente a la otra, produciendo un evidente sesgo, pero\n",
    "fallando completamente en detectar fraudes. Los impactos son: <br><br>\n",
    "1. Sesgo: El modelo ignora completamente la clase minoritaria (fraudes).\n",
    "2. Métricas engañosas: Aunque la precisión es alta (98%), el modelo\n",
    "tiene un Recall del 0% para la clase 1.\n",
    "3. Problema principal: No puede detectar fraudes, lo que lo hace inútil\n",
    "para su propósito. <br><br>\n",
    "4.\n",
    "Para poder mitigar este problema podemos aplicar técnicas como:\n",
    " <br><br>\n",
    "1. Oversampling con SMOTE para aumentar la cantidad de ejemplos\n",
    "de fraudes.\n",
    "2. Undersampling para reducir ejemplos de transacciones legítimas.\n",
    "3. Usar métricas más adecuadas, como F1-score o ROC-AUC, para\n",
    "evaluar el modelo"
   ],
   "id": "73bc74308af91aef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Datos simulados\n",
    "X, y = ... # Transacciones y etiquetas (0 = No fraude, 1 = Fraude)\n",
    "# SMOTE para balancear\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "# Entrenamiento y evaluación del modelo\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_resampled, y_resampled)\n",
    "y_pred = clf.predict(X)\n",
    "print(classification_report(y, y_pred))\n"
   ],
   "id": "c2c0ec19ec61ca9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Las técnicas de <b>oversampling</b> y <b>undersampling</b> se utilizan para abordar el\n",
    "problema de conjuntos de datos desbalanceados, en los que una o más clases\n",
    "tienen significativamente menos ejemplos que otras. Estas técnicas ajustan la\n",
    "distribución de clases para mejorar el rendimiento de los modelos supervisados.\n",
    "En **scikit-learn**, estas técnicas se implementan a menudo mediante la biblioteca\n",
    "**imbalanced-learn (imblearn).** <br><br>\n",
    "Además de SMOTE, también podemos emplear como alternativas:<br><br>\n",
    "1. **Random Oversampling:** duplica las muestras de la clase minoritaria al\n",
    "azar.\n",
    "2. **ADASYN:** similar a SMOTE, pero genera ejemplos basados en la\n",
    "densidad de las muestras minoritarias."
   ],
   "id": "e03aa6d5519e3c76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "oversampler = RandomOverSampler(random_state=42)\n",
    "adasyn = ADASYN(random_state=42)\n"
   ],
   "id": "b7c4f67751500626"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si no queremos depender de otras librerías, sklearn también nos ofrece una\n",
    "alternativa para realizar “resampling”, duplicando las muestras de la clase\n",
    "minoritaria o seleccionando aleatoriamente una parte de las muestras de la\n",
    "clase mayoritaria. Veamos un ejemplo conciso:\n"
   ],
   "id": "201a287cad709a2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Supongamos que separamos clases mayoritaria y minoritaria\n",
    "df_major = df[df['target'] == 0]\n",
    "df_minor = df[df['target'] == 1]\n",
    "# Ahora hacemos oversampling de la clase minoritaria\n",
    "df_minor_oversampled = resample(df_minor,\n",
    "replace=True,n_samples=len(df_major), random_state=42)\n",
    "# Combinar las clases\n",
    "df_balanced = pd.concat([df_major, df_minor_oversampled])\n"
   ],
   "id": "bc5d42697e34f602"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sin embargo, hay que tener en cuenta que esta técnica es más simple y por ello:\n",
    "1. **No genera datos sintéticos:** solo duplica o elimina datos existentes.\n",
    "2. **Mayor riesgo de overfitting en oversampling:** duplicar datos reales\n",
    "puede hacer que el modelo se ajuste demasiado a las muestras repetidas.\n",
    "3. **Sin manejo de ruido:** técnicas como SMOTE o ADASYN generan\n",
    "muestras más variadas, mientras que resample solo replica o elimina\n",
    "datos."
   ],
   "id": "76e2d8af7fb64a46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9.2. Tuning de hiperparámetros\n",
    "El tuning o ajuste de hiperparámetros es un paso crucial para optimizar el\n",
    "rendimiento de un modelo de ML. Los hiperparámetros son parámetros de\n",
    "configuración que debemos definir previamente. Estos pueden influir\n",
    "significativamente en la capacidad del modelo para generalizar y obtener buenos\n",
    "resultados en datos nuevos. <br><br>\n",
    "Los hiperparámetros suelen ser valores configurables en cada modelo de ML,\n",
    "que suelen indicarle al modelo cómo debe realizar el entrenamiento. Por ejemplo:<br><br>\n",
    "• **learning_rate (tasa de aprendizaje):** controla cuánto se ajustan los\n",
    "pesos del modelo con cada iteración.\n",
    "• **max_depth:** profundidad máxima de un árbol.\n",
    "• **n_estimators:** número de árboles en modelos como Random Forest o\n",
    "XGBoost. <br><br>\n",
    "El tuning busca encontrar la configuración óptima de hiperparámetros que\n",
    "maximice el rendimiento del modelo en datos nuevos, mejore las métricas o el\n",
    "tiempo de entrenamiento. Ejemplo: en un Random Forest, un número muy bajo\n",
    "de árboles puede resultar en un modelo inestable, mientras que un número muy\n",
    "alto puede aumentar el tiempo de cómputo sin mejorar el rendimiento."
   ],
   "id": "85a3d2cf45ac408d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Estrategias para el Tuning de Hiperparámetros**<br><br>\n",
    "**1. Grid Search:** consiste en definir un rango de valores para cada\n",
    "hiperparámetro y probar todas las combinaciones posibles de forma exhaustiva.\n",
    "Es ideal para datasets pequeños o pocos hiperparámetros por su alto coste\n",
    "computacional.<br><br>\n",
    "**Implementación con GridSearchCV en Scikit-learn:**"
   ],
   "id": "df8fafd76a6bdd80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = RandomForestClassifier()\n",
    "# Definir el espacio de hiperparámetros\n",
    "param_grid = {\n",
    " 'n_estimators': [100, 200, 300],\n",
    " 'max_depth': [5, 10, 15],\n",
    " 'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# Configurar la búsqueda y ver resultados\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5,\n",
    "scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n"
   ],
   "id": "8561d6e2f130515b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2. Random Search:** prueba combinaciones aleatorias dentro de los rangos\n",
    "definidos para cada hiperparámetro. Es eficiente en problemas con muchos\n",
    "hiperparámetros, pero no explora todas las combinaciones posibles.<br><br>\n",
    "**Implementación con RandomizedSearchCV en Scikit-learn:**\n"
   ],
   "id": "bcb8ae5aa1d395e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Espacio de hiperparámetros\n",
    "param_dist = {\n",
    " 'n_estimators': [100, 200, 300],\n",
    " 'max_depth': [5, 10, 15],\n",
    " 'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# Configurar la búsqueda y ver resultados\n",
    "random_search = RandomizedSearchCV(estimator=model,\n",
    "param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Mejores hiperparámetros:\", random_search.best_params_)"
   ],
   "id": "44ccbe7025d63844"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3. Búsqueda Bayesiana:** utiliza modelos probabilísticos para encontrar\n",
    "combinaciones óptimas de hiperparámetros de manera iterativa. Reduce el\n",
    "número de evaluaciones necesarias y es más eficiente que los anteriores en\n",
    "problemas complejos.<br><br>\n",
    "**Ejemplo con Optuna:**\n"
   ],
   "id": "19ae9f28b970d7dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Definir la función objetivo\n",
    "def objective(trial):\n",
    " n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    " max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    " min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    " model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "max_depth=max_depth, min_samples_split=min_samples_split)\n",
    " return cross_val_score(model, X_train, y_train, cv=5,\n",
    "scoring='accuracy').mean()\n",
    "# Crear el studio y ver los resultados\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "print(\"Mejores hiperparámetros:\", study.best_params)"
   ],
   "id": "363dcdf2f5970763"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9.3. Regularización\n",
   "id": "fb96e04f54be784d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9.4. Validación cruzada\n",
    "La validación cruzada (Cross-Validation) es una técnica utilizada en ML para\n",
    "evaluar el rendimiento de un modelo y evitar problemas como el sobreajuste\n",
    "(overfitting) o subajuste (underfitting). Consiste en dividir el conjunto de datos en\n",
    "múltiples particiones o \"folds\" y entrenar y evaluar el modelo varias veces,\n",
    "utilizando cada fold como conjunto de prueba una vez.<br><br>\n",
    "Existen varios métodos de validación cruzada que se utilizan según el problema\n",
    "y el tipo de datos:\n",
    "1. K-Fold Cross-Validation <br>\n",
    "Es el método más común. El conjunto de datos se divide en K partes (o \"folds\")\n",
    "de tamaño similar, y se entrena K veces, cada vez utilizando K-1 folds como\n",
    "entrenamiento y el fold restante como prueba. Al final, las métricas se promedian\n",
    "para obtener el rendimiento final. <br>\n",
    "2. Stratified K-Fold Cross-Validation <br>\n",
    "Es una variación del K-Fold. Se asegura de que cada fold tenga una distribución\n",
    "similar de las clases, lo cual es importante en problemas desbalanceados. Es\n",
    "útil en clasificación. <br><br>\n",
    "\n",
    "<img src=\"img/captura3.png\" alt=\"captura3\"> <br><br>\n",
    "\n",
    "Veamos un ejemplo de implementación de KFold:"
   ],
   "id": "597df053f66fea58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelo\n",
    "model = RandomForestClassifier()\n",
    "# Validación cruzada\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Scores por fold:\", scores)\n",
    "print(\"Media de accuracy:\", scores.mean())"
   ],
   "id": "a4aa6b3b84beafa2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9.5. Ensemble learning (modelos ensamblados)\n",
    "Los modelos ensamblados son técnicas avanzadas en ML que combinan\n",
    "múltiples modelos base para mejorar la precisión y robustez del modelo final. La\n",
    "idea principal es que la combinación de varios modelos, aunque individualmente\n",
    "no sean perfectos, puede dar lugar a un modelo mucho más sólido que los\n",
    "modelos individuales. <br><br>\n",
    "**Técnicas principales** <br><br>\n",
    "**1. Bagging (Bootstrap Aggregating)** <br><br>\n",
    "Bagging crea múltiples subconjuntos de los datos de entrenamiento usando\n",
    "muestreo con reemplazo (bootstrap sampling). Cada subconjunto entrena un\n",
    "modelo base, y la predicción final se obtiene promediando (para regresión) o\n",
    "mediante votación (para clasificación) de los resultados de todos los modelos.<br><br>\n",
    "**Ejemplo:** Random Forest Random Forest es un ejemplo clásico de bagging\n",
    "donde se construyen múltiples árboles de decisión independientes, y el resultado\n",
    "final es la votación mayoritaria o el promedio de las predicciones de todos los\n",
    "árboles. <br><br>\n",
    "\n",
    "<img src=\"img/captura4.png\" alt=\"captura4\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**2. Boosting:** combina modelos secuencialmente. Cada modelo se entrena\n",
    "tratando de corregir los errores cometidos por el modelo anterior. Al final, se\n",
    "combinan los modelos con pesos que dependen de su precisión. <br><br>\n",
    "**Ejemplos:** <br><br>\n",
    "a) **AdaBoost (Adaptive Boosting):** entrena modelos débiles\n",
    "secuencialmente y asigna pesos más altos a los errores en cada iteración. <br><br>\n",
    "b) **Gradient Boosting:** entrena los modelos secuenciales minimizando la\n",
    "función de pérdida en cada paso. <br><br>\n",
    "c) **XGBoost, LightGBM y CatBoost:** implementaciones optimizadas de\n",
    "boosting que permiten entrenar modelos de manera más eficiente y\n",
    "rápida. <br><br>\n",
    "\n",
    "<img src=\"img/captura5.png\" alt=\"captura5\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**3. Stacking (Stacked Generalization)** <br><br>\n",
    "Stacking combina diferentes tipos de modelos base (no necesariamente del\n",
    "mismo tipo) y utiliza un meta-modelo para aprender cómo combinar las\n",
    "predicciones de los modelos base. Permite aprovechar lo mejor de varios tipos\n",
    "de algoritmos, y es más flexible y potente que bagging y boosting; pero como\n",
    "contrapartida, tiene mayor complejidad y, por tanto, el tiempo de entrenamiento\n",
    "es más largo. <br><br>\n",
    "\n",
    "<img src=\"img/captura6.png\" alt=\"captura6\">\n",
    "\n",
    "<br><br>\n"
   ],
   "id": "c30dd1edec704634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelos base\n",
    "basemodels = [\n",
    " ('svc', SVC(kernel='linear', probability=True, random_state=42)),\n",
    " ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "# Meta-modelo: regresión logística\n",
    "model = StackingClassifier(estimators=basemodels,\n",
    "final_estimator=LogisticRegression())\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "ce29b74081b683f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Voting Classifier:** en **voting**, se combinan las predicciones de varios modelos\n",
    "para hacer una predicción final. Hay dos tipos de votación: <br><br>\n",
    "• **Votación dura (Hard Voting):** La predicción final se basa en la mayoría\n",
    "de votos. <br><br>\n",
    "• **Votación suave (Soft Voting):** Se promedian las probabilidades de\n",
    "predicción. <br><br>"
   ],
   "id": "6d5f3695f4c4a419"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelos individuales\n",
    "model1 = LogisticRegression()\n",
    "model2 = SVC(probability=True)\n",
    "model3 = DecisionTreeClassifier()\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "('lr', model1), ('svc', model2), ('dt', model3)\n",
    "], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ],
   "id": "5202785ef8e5c926"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Modelo de clasificación\n",
    "Basándonos en el mismo dataset con el que estamos trabajando, vamos ahora\n",
    "a abordar la creación de un modelo de clasificación, otro de los tipos de\n",
    "algoritmos supervisados más relevantes. Cambiaremos nuestro objetivo por otro,\n",
    "en esta ocasión queremos poder predecir la existencia de enfermedad cardíaca\n",
    "en base al resto de los atributos del dataset. <br>\n",
    "Por tanto, ahora, nuestra variable objetivo o “target” será: “cardio”.\n",
    "Aprovecharemos para ver otras formas de abordar algunos procedimientos\n",
    "vistos en el desarrollo del ejemplo de regresión.\n"
   ],
   "id": "ce64d37426d05c56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.1. Codificación y estandarización con sklearn\n",
    "La codificación de “gender” ahora la haremos mediante un LabelEncoder, que es\n",
    "una herramienta del módulo sklearn.preprocessing que se utiliza para convertir\n",
    "etiquetas de datos categóricos en números enteros, y lo hace convirtiendo cada\n",
    "categoría única de una variable en un número entero secuencial. Hay que tener\n",
    "en cuenta que es ideal para etiquetas de salida (target) y no tanto para variables\n",
    "independientes (features), para las cuales es mejor usar OneHotEncoder. Aún\n",
    "así vamos a probar con LabelEncoder y dejamos como propuesta contrastar los\n",
    "resultados codificando con OneHotEncoder.<br>\n",
    "Veamos cómo se haría:"
   ],
   "id": "b7391d331708ce34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.557376Z",
     "start_time": "2024-12-18T18:05:58.644344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar datos de CSV en variable \"datos\"\n",
    "datos = pd.read_csv('csvs/datos_salud2.csv')\n",
    "# Guardarnos en una variable el nombre de columna objetivo \"ap_hi\"\n",
    "valor_objetivo = 'ap_hi'\n",
    "# Reemplazo de edades nulas por la media\n",
    "datos['age'] = datos['age'].fillna(datos['age'].mean())\n",
    "# Reemplazo de géneros nulos por la moda\n",
    "datos['gender'] = datos['gender'].fillna(datos['gender'].mode()[0])\n",
    "# Eliminación de cardios nulos\n",
    "datos = datos.dropna(subset=['cardio'])\n",
    "datos = datos.astype({'age': 'int32', 'height': 'float32',\n",
    " 'weight': 'float32', 'ap_hi': 'int32', 'ap_lo': 'int32'})\n",
    "# Codificación \"one hot\" de la columna \"gender\"\n",
    "datos = pd.get_dummies(datos, columns=['gender'], prefix='Gender')\n",
    "# Codificaciones \"label encoding\" de \"cholesterol\" y \"gluc\"\n",
    "datos['cholesterol'] = datos['cholesterol'].replace(['Normal', 'Above Normal',\n",
    " 'Well Above Normal'], [0, 1, 2])\n",
    "datos['gluc'] = datos['gluc'].replace(['Normal', 'Above Normal',\n",
    " 'Well Above Normal'], [0, 1, 2])\n",
    "# Codificaciones binarias de columnas SI/NO\n",
    "datos['smoke'] = datos['smoke'].replace(['No', 'Yes'], [0, 1])\n",
    "datos['alco'] = datos['alco'].replace(['No', 'Yes'], [0, 1])\n",
    "datos['active'] = datos['active'].replace(['No', 'Yes'], [0, 1])\n",
    "datos['cardio'] = datos['cardio'].replace(['No', 'Yes'], [0, 1])\n",
    "le = LabelEncoder()\n",
    "datos['gender'] = le.fit_transform(datos['gender'])"
   ],
   "id": "a23b10b8b7448e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_12544\\3450702225.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datos = pd.read_csv('csvs/datos_salud2.csv')\n",
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_12544\\3450702225.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datos['cholesterol'] = datos['cholesterol'].replace(['Normal', 'Above Normal',\n",
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_12544\\3450702225.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datos['gluc'] = datos['gluc'].replace(['Normal', 'Above Normal',\n",
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_12544\\3450702225.py:21: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datos['smoke'] = datos['smoke'].replace(['No', 'Yes'], [0, 1])\n",
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_12544\\3450702225.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datos['alco'] = datos['alco'].replace(['No', 'Yes'], [0, 1])\n",
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_12544\\3450702225.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datos['active'] = datos['active'].replace(['No', 'Yes'], [0, 1])\n",
      "C:\\Users\\ESP\\AppData\\Local\\Temp\\ipykernel_12544\\3450702225.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datos['cardio'] = datos['cardio'].replace(['No', 'Yes'], [0, 1])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gender'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'gender'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcardio\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcardio\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreplace([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYes\u001B[39m\u001B[38;5;124m'\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     25\u001B[0m le \u001B[38;5;241m=\u001B[39m LabelEncoder()\n\u001B[1;32m---> 26\u001B[0m datos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m le\u001B[38;5;241m.\u001B[39mfit_transform(\u001B[43mdatos\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgender\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3810\u001B[0m     ):\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'gender'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "El escalado y normalización, en esta ocasión, lo haremos con un StandardScaler.\n",
    "Se trata de una clase de scikit-learn que se utiliza para escalar características\n",
    "(features) en un conjunto de datos a una distribución con media 0 y desviación\n",
    "estándar 1. Es una de las técnicas más comunes para la normalización de datos\n",
    "en Machine Learning, ya que muchas veces los algoritmos funcionan mejor\n",
    "cuando las características están estandarizadas.<br><br>\n",
    "StandardScaler transforma cada característica X (columna de datos) según la\n",
    "siguiente fórmula:<br><br>\n",
    "\n",
    "<img src=\"img/captura1.png\" alt=\"captura1\"> <br><br>\n",
    "\n",
    "Donde: <br>\n",
    "• 𝑍: valor estandarizado (output).<br>\n",
    "• 𝑋: valor original de la característica.<br>\n",
    "• 𝜇: media de la característica (calculada en el conjunto de\n",
    "entrenamiento).<br>\n",
    "• 𝜎: desviación estándar de la característica (calculada en el conjunto de\n",
    "entrenamiento).<br>\n",
    "Veamos cómo se haría (reflexiona: ¿se te ocurre alguna forma de optimizar este\n",
    "bloque de código?):\n",
    "\n"
   ],
   "id": "14a8f194e6f31042"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.563488100Z",
     "start_time": "2024-12-18T18:03:50.263037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "se = StandardScaler()\n",
    "datos['age'] = se.fit_transform(datos[['age']])\n",
    "datos['weight'] = se.fit_transform(datos[['weight']])\n",
    "datos['height'] = se.fit_transform(datos[['height']])\n",
    "datos['ap_hi'] = se.fit_transform(datos[['ap_hi']])\n",
    "datos['ap_lo'] = se.fit_transform(datos[['ap_lo']])\n"
   ],
   "id": "d52583514f432646",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<b>Recuerda:</b> no es necesario escalar ni normalizar los datos cuando usas\n",
    "algoritmos basados en árboles de decisión y sus derivados, como Random\n",
    "Forest, Gradient Boosting (e.g., XGBoost, LightGBM) o AdaBoost."
   ],
   "id": "16042f9685f75cea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.2. Preparación de los conjuntos train y test\n",
    "Una vez hemos realizado la conversión de categorías a variables numéricas y\n",
    "hemos estandarizado, podemos continuar preparando los conjuntos de\n",
    "entrenamiento y validación previos al entrenamiento de los diferentes algoritmos.<br><br>\n",
    "Prepararemos nuevos dataframes con las variables de decisión (X) y el de la\n",
    "variable target (y), así como los conjuntos de entrenamiento y validación:\n"
   ],
   "id": "48906b3c125ae06f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.566752300Z",
     "start_time": "2024-12-18T18:03:52.544266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = datos.drop('cardio',axis=1)\n",
    "y = datos['cardio']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)"
   ],
   "id": "96482d4091b36cd3",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.3. Entrenamiento y validación de modelos de clasificación\n",
    "Dado que ahora hemos pasado a un modelo de clasificación, las métricas a usar\n",
    "son diferentes al caso de regresión. Para trabajar más cómodamente vamos a\n",
    "crear una función llamada “train_validation” que se va a encargar de: <br><br>\n",
    "1. Entrenar el modelo con los conjuntos de train.\n",
    "2. Probar el modelo con el conjunto de validación.\n",
    "3. Mostrar las métricas así como la matriz de confusión para valorar la\n",
    "calidad del modelo.<br><br>\n",
    "Veamos como quedaría nuestra función:\n"
   ],
   "id": "71ca068a17e354e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.570376200Z",
     "start_time": "2024-12-18T18:03:55.598370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_validation(model):\n",
    " model.fit(X_train,y_train.astype(int))\n",
    " y_pred = model.predict(X_test)\n",
    " conf_matrix = confusion_matrix(y_pred, y_test)\n",
    " print(classification_report(y_pred, y_test.astype(int)))\n",
    " print('score_test = ', model.score(X_test, y_test.astype(int)))\n",
    " print('score_train = ', model.score(X_train, y_train.astype(int)))\n",
    " plt.figure(figsize=(6, 4))\n",
    " sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    " xticklabels=[\"No Enfermo\", \"Enfermo\"], yticklabels=[\"No Enfermo\",\n",
    " \"Enfermo\"])\n",
    " plt.title(\"Matriz de Confusión\")\n",
    " plt.xlabel(\"Predicción\")\n",
    " plt.ylabel(\"Real\")\n",
    " plt.show()\n"
   ],
   "id": "f4134776704a0d00",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si probamos con un primer modelo de clasificación, por ejemplo, un\n",
    "RandomForestClassifier:\n"
   ],
   "id": "a85b165d884d062a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T19:35:25.574521200Z",
     "start_time": "2024-12-18T18:03:58.538961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "train_validation(model)"
   ],
   "id": "1b8d050d2475f0d",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Yes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m RandomForestClassifier(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[32], line 2\u001B[0m, in \u001B[0;36mtrain_validation\u001B[1;34m(model)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_validation\u001B[39m(model):\n\u001B[1;32m----> 2\u001B[0m  model\u001B[38;5;241m.\u001B[39mfit(X_train,\u001B[43my_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      3\u001B[0m  y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      4\u001B[0m  conf_matrix \u001B[38;5;241m=\u001B[39m confusion_matrix(y_pred, y_test)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001B[0m, in \u001B[0;36mNDFrame.astype\u001B[1;34m(self, dtype, copy, errors)\u001B[0m\n\u001B[0;32m   6637\u001B[0m     results \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   6638\u001B[0m         ser\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39mcopy, errors\u001B[38;5;241m=\u001B[39merrors) \u001B[38;5;28;01mfor\u001B[39;00m _, ser \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   6639\u001B[0m     ]\n\u001B[0;32m   6641\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6642\u001B[0m     \u001B[38;5;66;03m# else, only a single dtype is given\u001B[39;00m\n\u001B[1;32m-> 6643\u001B[0m     new_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6644\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor_from_mgr(new_data, axes\u001B[38;5;241m=\u001B[39mnew_data\u001B[38;5;241m.\u001B[39maxes)\n\u001B[0;32m   6645\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mastype\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001B[0m, in \u001B[0;36mBaseBlockManager.astype\u001B[1;34m(self, dtype, copy, errors)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m using_copy_on_write():\n\u001B[0;32m    428\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 430\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mastype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_cow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musing_copy_on_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001B[0m, in \u001B[0;36mBaseBlockManager.apply\u001B[1;34m(self, f, align_keys, **kwargs)\u001B[0m\n\u001B[0;32m    361\u001B[0m         applied \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mapply(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 363\u001B[0m         applied \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    364\u001B[0m     result_blocks \u001B[38;5;241m=\u001B[39m extend_blocks(applied, result_blocks)\n\u001B[0;32m    366\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mfrom_blocks(result_blocks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001B[0m, in \u001B[0;36mBlock.astype\u001B[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001B[0m\n\u001B[0;32m    755\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not squeeze with more than one column.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    756\u001B[0m     values \u001B[38;5;241m=\u001B[39m values[\u001B[38;5;241m0\u001B[39m, :]  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m--> 758\u001B[0m new_values \u001B[38;5;241m=\u001B[39m \u001B[43mastype_array_safe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    760\u001B[0m new_values \u001B[38;5;241m=\u001B[39m maybe_coerce_values(new_values)\n\u001B[0;32m    762\u001B[0m refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001B[0m, in \u001B[0;36mastype_array_safe\u001B[1;34m(values, dtype, copy, errors)\u001B[0m\n\u001B[0;32m    234\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtype\u001B[38;5;241m.\u001B[39mnumpy_dtype\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 237\u001B[0m     new_values \u001B[38;5;241m=\u001B[39m \u001B[43mastype_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mValueError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m):\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001B[39;00m\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m#  trying to convert to float\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001B[0m, in \u001B[0;36mastype_array\u001B[1;34m(values, dtype, copy)\u001B[0m\n\u001B[0;32m    179\u001B[0m     values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39mcopy)\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 182\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[43m_astype_nansafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, np\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(values\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mtype, \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001B[0m, in \u001B[0;36m_astype_nansafe\u001B[1;34m(arr, dtype, copy, skipna)\u001B[0m\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mor\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m dtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m:\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001B[39;00m\n\u001B[1;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "\u001B[1;31mValueError\u001B[0m: invalid literal for int() with base 10: 'Yes'"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<b>Reflexiona:</b> ¿qué podemos deducir de las métricas obtenidas?, ¿crees que es\n",
    "un buen modelo?"
   ],
   "id": "6bfac219d7ae24d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
